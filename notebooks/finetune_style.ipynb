{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KumYPYVVzhJt"
      },
      "source": [
        "# Fine-tuning a Language Model for Custom-Style Text Generation\n",
        "\n",
        "This notebook demonstrates how to fine-tune a language model to generate text in a custom-style voice. We'll use a dataset of paired emails (standard and custom-style) to teach the model how to transform regular text into custom speech.\n",
        "\n",
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTBp8A450cnw",
        "outputId": "c8d1766a-26e8-467f-8035-32529f6cc1ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bc8yICh0c8k",
        "outputId": "4ff3412c-3343-4622-dc45-4231d4981a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'humanize-LLM' already exists and is not an empty directory.\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.50.0)\n",
            "Requirement already satisfied: datasets>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: evaluate>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.4.3)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.45.4)\n",
            "Requirement already satisfied: peft>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: trl>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: wandb>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.19.8)\n",
            "Requirement already satisfied: optimum>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (1.24.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.29.3)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (2.0.2)\n",
            "Requirement already satisfied: tensorboard>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (2.18.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (4.67.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (0.13.2)\n",
            "Requirement already satisfied: pytest>=7.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (8.3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (3.11.14)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.3->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl>=0.2.2->-r requirements.txt (line 8)) (13.9.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (2.24.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (75.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (3.7)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 16)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 16)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 16)) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (3.2.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r requirements.txt (line 20)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r requirements.txt (line 20)) (1.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 9)) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.15.0->-r requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.15.0->-r requirements.txt (line 9)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.29.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.29.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.29.0->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.29.0->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.13.0->-r requirements.txt (line 14)) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl>=0.2.2->-r requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl>=0.2.2->-r requirements.txt (line 8)) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 9)) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl>=0.2.2->-r requirements.txt (line 8)) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install \"torch==2.0.1\" \"transformers==4.34.0\" \"datasets==2.14.5\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.7.2\" \"peft==0.5.0\" \"tensorboard\" \"flash-attn\" --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF8_x5vMzhJz"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/TheBormann/humanize-LLM.git\n",
        "!cd humanize-LLM && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "# Add the parent directory to the path\n",
        "sys.path.append('/content/humanize-LLM')\n",
        "\n",
        "# Import TRL components for efficient fine-tuning\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    stream=sys.stdout\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lox5td4izhJ0"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "We'll load our dataset of paired emails from a CSV file, but now we'll convert it to the modern conversational format for better fine-tuning with TRL's SFTTrainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c__CwiEmzhJ0"
      },
      "outputs": [],
      "source": [
        "def load_emails_from_csv(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load emails from a CSV file with semicolon delimiter.\"\"\"\n",
        "    df = pd.read_csv(file_path, sep=';')\n",
        "    logger.info(f\"Loaded {len(df)} emails from {file_path}\")\n",
        "    return df\n",
        "\n",
        "def prepare_training_data(emails_df: pd.DataFrame) -> List[Dict]:\n",
        "    \"\"\"Prepare training data in conversational format for SFTTrainer.\n",
        "\n",
        "    Creates direct style transfer pairs in conversational format:\n",
        "    - system: instruction on style transformation\n",
        "    - user: original AI-generated email\n",
        "    - assistant: styled version\n",
        "    \"\"\"\n",
        "    system_message = \"\"\"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone. \n",
        "Your goal is to make the text feel more human-written with natural speech patterns.\"\"\"\n",
        "\n",
        "    training_samples = []\n",
        "\n",
        "    for _, row in emails_df.iterrows():\n",
        "        if pd.isna(row['body']) or pd.isna(row['body_ai']):\n",
        "            continue\n",
        "\n",
        "        # Create conversation in the format expected by TRL's SFTTrainer\n",
        "        sample = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": row['body_ai']},  # AI-generated email\n",
        "                {\"role\": \"assistant\", \"content\": row['body']}  # Custom style version\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        training_samples.append(sample)\n",
        "\n",
        "    logger.info(f\"Created {len(training_samples)} conversational training samples\")\n",
        "    return training_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDVh-N2kzhJ1",
        "outputId": "108ec701-a1f2-4811-dc25-bc0790beb0ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample training pair:\n",
            "Prompt (AI-generated): Hi [Name],\\n\\nI'm [Your Name], founder of [Startup Name]. We're revolutionizing [industry] through [key innovation]. Would you have time next week to ...\n",
            "Response (custom-style): Ahoy [Name],\\n\\nYer lookin' at [Your Name], fearsome captain of [Startup Name]. We be chartin' treacherous waters of [industry] with [key innovation] ...\n",
            "Total training pairs: 69\n"
          ]
        }
      ],
      "source": [
        "# Set the path to the CSV file\n",
        "EMAIL_CSV_PATH = '/content/humanize-LLM/data/manual_emails.csv'\n",
        "\n",
        "# Load and prepare the dataset\n",
        "emails_df = load_emails_from_csv(EMAIL_CSV_PATH)\n",
        "training_data = prepare_training_data(emails_df)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "dataset = Dataset.from_list(training_data)\n",
        "\n",
        "# Display a sample of the training data\n",
        "if len(dataset) > 0:\n",
        "    sample = dataset[0]\n",
        "    print(\"Sample training conversation:\")\n",
        "    for message in sample['messages']:\n",
        "        print(f\"{message['role']}: {message['content'][:100]}...\")\n",
        "    print(f\"Total training samples: {len(dataset)}\")\n",
        "else:\n",
        "    print(\"No training data found or prepared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7XNF6y0zhJ1"
      },
      "source": [
        "## Model Selection and QLoRA Configuration\n",
        "\n",
        "We'll use a smaller model suitable for Google Colab (Mistral-7B-Instruct-v0.2) with QLoRA for efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # A smaller but capable model\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/custom_style_model\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# QLoRA Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_ratio=0.1,\n",
        "    bf16=True,  # Use mixed precision\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning with SFTTrainer and QLoRA\n",
        "\n",
        "We'll use the SFTTrainer from TRL with QLoRA for parameter-efficient fine-tuning, significantly reducing memory requirements while maintaining performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_prepare_model():\n",
        "    \"\"\"Load and prepare the model for QLoRA fine-tuning\"\"\"\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load model with quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Prepare model for kbit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Print trainable parameters info\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAQjx1gqzhJ1"
      },
      "outputs": [],
      "source": [
        "def finetune_model(dataset):\n",
        "    \"\"\"Fine-tune model using SFTTrainer with QLoRA\"\"\"\n",
        "    logger.info(\"Loading and preparing model...\")\n",
        "    model, tokenizer = load_and_prepare_model()\n",
        "\n",
        "    logger.info(\"Initializing SFTTrainer...\")\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        args=training_args,\n",
        "        max_seq_length=1024,\n",
        "        dataset_text_field=\"messages\",  # Use our conversation format\n",
        "        packing=True  # Enable packing for more efficient training\n",
        "    )\n",
        "\n",
        "    logger.info(\"Starting fine-tuning...\")\n",
        "    trainer.train()\n",
        "\n",
        "    logger.info(f\"Saving model to {OUTPUT_DIR}\")\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "a804f29419b14e49872288ffb5bcb6c8",
            "4867e2aecc1048c6b195e07ed1965a31",
            "0c51211149b84dd3ab13c5dc56b1b9d9",
            "6ae57f3754e24e519d40870f174644cf",
            "124b2bc92d084cfeaea69ca599d38b3b",
            "1a69a2014b75456fb6907a979e3a4c75",
            "2953e34a1cef442f9caca67a1e121fde",
            "339a68b5941e4f34bb9da94b59bfc2b4",
            "030fcc792cc24b6db93fca975fc76b6d",
            "fc789e53a29d4ceebf1375170a3c280a",
            "43a33b08d6b64f11966d30cb73e8a415"
          ]
        },
        "id": "zBq3FWyXzhJ2",
        "outputId": "afa3efab-4b32-429b-9cba-9e1cda165c05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a804f29419b14e49872288ffb5bcb6c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Run the fine-tuning\n",
        "model, tokenizer = finetune_model(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLIfAqFtzhJ2"
      },
      "source": [
        "## Test and Evaluate the Fine-tuned Model\n",
        "\n",
        "Let's test our fine-tuned model with some example prompts and implement proper evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to generate responses with our fine-tuned model\n",
        "def generate_styled_text(prompt, model, tokenizer, max_new_tokens=200):\n",
        "    \"\"\"Generate styled text from prompt using our fine-tuned model\"\"\"\n",
        "    # Prepare conversation for inference\n",
        "    system_message = \"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Format with chat template\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and extract only the generated part\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    assistant_response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    return assistant_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQjAlOrGzhJ2"
      },
      "outputs": [],
      "source": [
        "# Test with different prompts\n",
        "import torch\n",
        "\n",
        "test_prompts = [\n",
        "    \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\",\n",
        "    \"Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience.\",\n",
        "    \"Team, please remember to submit your reports by Friday. The client is expecting our analysis.\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nTest Prompt {i+1}:\\n{prompt}\")\n",
        "    styled_response = generate_styled_text(prompt, model, tokenizer)\n",
        "    print(f\"\\nCustom-Style Response:\\n{styled_response}\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "Let's evaluate our model on a subset of emails not used for training to assess its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, test_samples=5):\n",
        "    \"\"\"Evaluate model performance on test samples from the dataset\"\"\"\n",
        "    # Use a subset of our dataset for testing\n",
        "    if len(dataset) <= test_samples:\n",
        "        test_indices = range(len(dataset))\n",
        "    else:\n",
        "        import random\n",
        "        test_indices = random.sample(range(len(dataset)), test_samples)\n",
        "\n",
        "    print(f\"\\nEvaluating model on {len(test_indices)} test samples...\")\n",
        "\n",
        "    for idx in test_indices:\n",
        "        sample = dataset[idx]\n",
        "\n",
        "        # Extract original prompt and reference\n",
        "        original_text = sample['messages'][1]['content']  # user message\n",
        "        reference_text = sample['messages'][2]['content'] # assistant message\n",
        "\n",
        "        # Generate styled version\n",
        "        generated_text = generate_styled_text(original_text, model, tokenizer)\n",
        "\n",
        "        print(f\"\\nOriginal: {original_text[:150]}...\")\n",
        "        print(f\"\\nGenerated: {generated_text[:150]}...\")\n",
        "        print(f\"\\nReference: {reference_text[:150]}...\")\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_model(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge Adapter Weights (Optional)\n",
        "\n",
        "For deployment, you might want to merge the LoRA adapter weights back into the base model for more efficient inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_adapter_weights():\n",
        "    \"\"\"Merge LoRA adapter weights into the base model\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "    # Load the fine-tuned PEFT model\n",
        "    peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        OUTPUT_DIR,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Merge weights\n",
        "    merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "    # Save the merged model\n",
        "    merged_model_path = f\"{OUTPUT_DIR}_merged\"\n",
        "    merged_model.save_pretrained(merged_model_path)\n",
        "    tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "    print(f\"Merged model saved to {merged_model_path}\")\n",
        "\n",
        "    return merged_model_path\n",
        "\n",
        "# Uncomment to merge weights\n",
        "# merged_model_path = merge_adapter_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-htzHMjzhJ2"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated how to fine-tune a language model to generate text in a specific style using modern, efficient techniques from 2025:\n",
        "\n",
        "1. We used QLoRA for parameter-efficient fine-tuning, which dramatically reduces the memory requirements\n",
        "2. We implemented the conversational format for better compatibility with SFTTrainer\n",
        "3. We applied optimizations like gradient checkpointing and mixed precision training\n",
        "4. We used a smaller but capable model (Mistral-7B) that fits on Google Colab's resources\n",
        "5. We incorporated proper evaluation techniques\n",
        "\n",
        "These approaches allow for efficient fine-tuning even with limited computational resources like those available on Google Colab, while still producing high-quality results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "030fcc792cc24b6db93fca975fc76b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c51211149b84dd3ab13c5dc56b1b9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_339a68b5941e4f34bb9da94b59bfc2b4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_030fcc792cc24b6db93fca975fc76b6d",
            "value": 1
          }
        },
        "124b2bc92d084cfeaea69ca599d38b3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a69a2014b75456fb6907a979e3a4c75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2953e34a1cef442f9caca67a1e121fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "339a68b5941e4f34bb9da94b59bfc2b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a33b08d6b64f11966d30cb73e8a415": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4867e2aecc1048c6b195e07ed1965a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a69a2014b75456fb6907a979e3a4c75",
            "placeholder": "​",
            "style": "IPY_MODEL_2953e34a1cef442f9caca67a1e121fde",
            "value": "Loading checkpoint shards:  33%"
          }
        },
        "6ae57f3754e24e519d40870f174644cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc789e53a29d4ceebf1375170a3c280a",
            "placeholder": "​",
            "style": "IPY_MODEL_43a33b08d6b64f11966d30cb73e8a415",
            "value": " 1/3 [00:20&lt;00:41, 20.99s/it]"
          }
        },
        "a804f29419b14e49872288ffb5bcb6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4867e2aecc1048c6b195e07ed1965a31",
              "IPY_MODEL_0c51211149b84dd3ab13c5dc56b1b9d9",
              "IPY_MODEL_6ae57f3754e24e519d40870f174644cf"
            ],
            "layout": "IPY_MODEL_124b2bc92d084cfeaea69ca599d38b3b"
          }
        },
        "fc789e53a29d4ceebf1375170a3c280a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
