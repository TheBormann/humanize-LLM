{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KumYPYVVzhJt"
      },
      "source": [
        "# Fine-tuning a Language Model for Custom-Style Text Generation\n",
        "\n",
        "This notebook demonstrates how to fine-tune a language model to generate text in a custom-style voice. We'll use a dataset of paired emails (standard and custom-style) to teach the model how to transform regular text into custom speech.\n",
        "\n",
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "E0eBce2IsSrI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "\n",
        "# Add the parent directory to the path - fixed for Jupyter notebook\n",
        "current_dir = os.getcwd()\n",
        "parent_dir = os.path.dirname(current_dir)\n",
        "sys.path.append(parent_dir)\n",
        "\n",
        "# Import TRL components for efficient fine-tuning\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    stream=sys.stdout\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lox5td4izhJ0"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "We'll load our dataset of paired emails from a CSV file, but now we'll convert it to the modern conversational format for better fine-tuning with TRL's SFTTrainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c__CwiEmzhJ0"
      },
      "outputs": [],
      "source": [
        "def load_emails_from_csv(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load emails from a CSV file with semicolon delimiter.\"\"\"\n",
        "    df = pd.read_csv(file_path, sep=';')\n",
        "    logger.info(f\"Loaded {len(df)} emails from {file_path}\")\n",
        "    return df\n",
        "\n",
        "def prepare_training_data(emails_df: pd.DataFrame) -> List[Dict]:\n",
        "    \"\"\"Prepare training data in conversational format for SFTTrainer.\n",
        "\n",
        "    Creates direct style transfer pairs in conversational format:\n",
        "    - system: instruction on style transformation\n",
        "    - user: original AI-generated email\n",
        "    - assistant: styled version\n",
        "    \"\"\"\n",
        "    system_message = \"\"\"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
        "Your goal is to make the text feel more human-written with natural speech patterns.\"\"\"\n",
        "\n",
        "    training_samples = []\n",
        "\n",
        "    for _, row in emails_df.iterrows():\n",
        "        if pd.isna(row['body']) or pd.isna(row['body_ai']):\n",
        "            continue\n",
        "\n",
        "        # Create conversation in the format expected by TRL's SFTTrainer\n",
        "        sample = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": row['body_ai']},  # AI-generated email\n",
        "                {\"role\": \"assistant\", \"content\": row['body']}  # Custom style version\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        training_samples.append(sample)\n",
        "\n",
        "    logger.info(f\"Created {len(training_samples)} conversational training samples\")\n",
        "    return training_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDVh-N2kzhJ1",
        "outputId": "99f968d5-f7ce-4cdb-80f8-4a0ea0e6b170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-30 00:39:12,078 - __main__ - INFO - Loaded 69 emails from ./data/manual_emails.csv\n",
            "2025-03-30 00:39:12,080 - __main__ - INFO - Created 69 conversational training samples\n",
            "Sample training conversation:\n",
            "system: Transform the given email into a custom-styled version that maintains the same content but uses a mo...\n",
            "user: Hi [Name],\\n\\nI'm [Your Name], founder of [Startup Name]. We're revolutionizing [industry] through [...\n",
            "assistant: Ahoy [Name],\\n\\nYer lookin' at [Your Name], fearsome captain of [Startup Name]. We be chartin' treac...\n",
            "Total training samples: 69\n"
          ]
        }
      ],
      "source": [
        "# Set the path to the CSV file\n",
        "EMAIL_CSV_PATH = './data/manual_emails.csv'\n",
        "\n",
        "# Load and prepare the dataset\n",
        "emails_df = load_emails_from_csv(EMAIL_CSV_PATH)\n",
        "training_data = prepare_training_data(emails_df)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "dataset = Dataset.from_list(training_data)\n",
        "\n",
        "# Display a sample of the training data\n",
        "if len(dataset) > 0:\n",
        "    sample = dataset[0]\n",
        "    print(\"Sample training conversation:\")\n",
        "    for message in sample['messages']:\n",
        "        print(f\"{message['role']}: {message['content'][:100]}...\")\n",
        "    print(f\"Total training samples: {len(dataset)}\")\n",
        "else:\n",
        "    print(\"No training data found or prepared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7XNF6y0zhJ1"
      },
      "source": [
        "## Model Selection and QLoRA Configuration\n",
        "\n",
        "We'll use a smaller model suitable for Google Colab (Mistral-7B-Instruct-v0.2) with QLoRA for efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Y7Pl6IsxsSrI"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # 2.7B parameters model from Microsoft\n",
        "OUTPUT_DIR = \"./output/custom_style_model\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# QLoRA Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_ratio=0.1,\n",
        "    bf16=True,  # Use mixed precision\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdmLqu6EsSrI"
      },
      "source": [
        "## Fine-tuning with SFTTrainer and QLoRA\n",
        "\n",
        "We'll use the SFTTrainer from TRL with QLoRA for parameter-efficient fine-tuning, significantly reducing memory requirements while maintaining performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Rpr7mcspsSrI"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_model():\n",
        "    \"\"\"Load and prepare model for fine-tuning on Mac\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load model without quantization for Mac compatibility\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,  # Use float16 for efficiency\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    # Prepare model for training\n",
        "    # Skip prepare_model_for_kbit_training since we're not using quantization\n",
        "    \n",
        "    # Configure LoRA adapter\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "    )\n",
        "    \n",
        "    # Apply LoRA adapter\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    \n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wAQjx1gqzhJ1"
      },
      "outputs": [],
      "source": [
        "def finetune_model(dataset):\n",
        "    # Load model and tokenizer\n",
        "    model, tokenizer = load_and_prepare_model()\n",
        "    \n",
        "    # Explicitly ensure model is in training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Verify parameters require gradients\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Trainable parameters: {trainable_params}\")\n",
        "    \n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        save_steps=200,\n",
        "        logging_steps=50,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,  # Using mixed precision\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "    \n",
        "    # Use DataCollatorForLanguageModeling for causal language models\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # Not using masked language modeling\n",
        "    )\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    \n",
        "    # Save the final model\n",
        "    model.save_pretrained(\"./fine_tuned_model\")\n",
        "    tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "    \n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461,
          "referenced_widgets": [
            "84530d834cf34728b943cbc1307ac3ef",
            "d85ea79fd8b64749b87b3830532437e2",
            "87e8fba41b88475a866234e92de246b9",
            "08fb02dfc3cb4926873600def54162dd",
            "bdbc774f19074ec48be5569382a70725",
            "16589004f6cf46399dea10351c03f75a",
            "41d23c8c4c014feda41ae704756b27fe",
            "b5afcbfe728c4238999fa9d1b0e44cbe",
            "537906d718e2443aba58389f73ad47cc",
            "5a1eae16356c4b7198654b93d84a5bfc",
            "c3c179a86176407eabf64648773512c4",
            "c4108b2bca78465a8f0ae9f1d74a5035",
            "85e7b03a3e164e44bf8183a5cbc3cd25",
            "8ed49a1df23a462e8602c5904c56cbc7",
            "0ad07ffbab4e4829a3ff8013081f1d88",
            "2dcd656d76f249448f21c3423fce2b35",
            "7c25715847ab45acab6ccb5de84ed7d0",
            "935bd2febc66455ab1aeb6f41f0ddcde",
            "c94effb3c5c74d6e89af78534c301b37",
            "05d36e9a13b244469880979a29940def",
            "a0d1b761e79848da86c8d712e224fd20",
            "6806c0b84852473aa560798671c26b11",
            "1d2643e1d4ec48bbb2a843eaa04aad55",
            "564c9338cc5a4b34a74d20037abd7157",
            "d1badf74bf554c61bdf1540d66d49bb4",
            "792456974b844899b8b838ee33c335bd",
            "d71154e731084b7da8b0dda47561ff79",
            "560d6364ab9b4e03885ced53a65ca405",
            "2c4859ee8e4a429ca489b3bf7144ec83",
            "2186b649bf2448faac5508f0f4c024bd",
            "cbabf3a9cfbf4e888df1c8f11e003ec4",
            "46152fbfd14f42e8b979280920e2ca1b",
            "996701bf1860407b99c9eb97891d121b",
            "30021af32c1644a7a6b36ed7abb08dd0",
            "db364d88095d4f4dab0754a548b01114",
            "1cc5d158a5294292bac6e3b0dac9f1f4",
            "16d59e6810b542f9a6993a3c851568b7",
            "bbcb0db9b29f4813bd34fbb6346a1917",
            "7e3cd9397f1a419a8cea5d807e1273f5",
            "22f5c46e907341dab0e6bda949b0ee13",
            "a27fe7d4ba6e4343ba7a480a618e6a28",
            "b182bd03ca5c4173ab43bad62023d9a1",
            "733056ec9ba44d8ea48c70e557261dcf",
            "0f992ef170aa4b4bb3f1c8b05db0d7ed",
            "11d152292b5d419db29b720af3ed17df",
            "475303ea81884c05a139800a90131ee8",
            "a8b9cd1b1f814cba8904e80b53d174f6",
            "db88865a74b14b2b90356136e20b005b",
            "f2ff8fa153404f4e9d2462d90b4323ea",
            "d36d4e59579046788804c2a5e4ca1cde",
            "d34cd591241647d78d79ab6268102a0f",
            "d564512ce9b64d2aa21b30feec02457c",
            "0f7120783c714a449593ec6f3daabdb1",
            "1891136a49214d3086e68468ce534eed",
            "06c323a0068e47818e81390f8290eab5"
          ]
        },
        "id": "zBq3FWyXzhJ2",
        "outputId": "ee9743ae-b75f-4757-efbb-23470df7301c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 4505600\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'Trainer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the fine-tuning\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model, tokenizer = \u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mfinetune_model\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m     26\u001b[39m data_collator = DataCollatorForLanguageModeling(\n\u001b[32m     27\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     28\u001b[39m     mlm=\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Not using masked language modeling\u001b[39;00m\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Create trainer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m trainer = \u001b[43mTrainer\u001b[49m(\n\u001b[32m     33\u001b[39m     model=model,\n\u001b[32m     34\u001b[39m     args=training_args,\n\u001b[32m     35\u001b[39m     train_dataset=dataset,\n\u001b[32m     36\u001b[39m     data_collator=data_collator,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     40\u001b[39m trainer.train()\n",
            "\u001b[31mNameError\u001b[39m: name 'Trainer' is not defined"
          ]
        }
      ],
      "source": [
        "# Run the fine-tuning\n",
        "model, tokenizer = finetune_model(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLIfAqFtzhJ2"
      },
      "source": [
        "## Test and Evaluate the Fine-tuned Model\n",
        "\n",
        "Let's test our fine-tuned model with some example prompts and implement proper evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft9FV1oVsSrJ"
      },
      "outputs": [],
      "source": [
        "# Function to generate responses with our fine-tuned model\n",
        "def generate_styled_text(prompt, model, tokenizer, max_new_tokens=200):\n",
        "    \"\"\"Generate styled text from prompt using our fine-tuned model\"\"\"\n",
        "    # Prepare conversation for inference\n",
        "    system_message = \"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Format with chat template\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and extract only the generated part\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Properly extract only the assistant's response\n",
        "    if \"<|assistant|>\" in generated_text:\n",
        "        assistant_response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
        "        # Remove any instruction tags that might be in the output\n",
        "        assistant_response = assistant_response.replace(\"[/INST]\", \"\").strip()\n",
        "    else:\n",
        "        # Fallback if assistant token isn't found\n",
        "        assistant_response = generated_text.split(prompt)[-1].strip()\n",
        "        assistant_response = assistant_response.replace(\"[/INST]\", \"\").strip()\n",
        "\n",
        "    return assistant_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQjAlOrGzhJ2",
        "outputId": "05b5b673-2abf-40c4-e543-15d6ee7f5bb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Prompt 1:\n",
            "Hello, I'm writing to inquire about your services. Could we schedule a call next week?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hello, I'm writing to inquire about your services. Could we schedule a call next week? [/INST] Ahoy! I be seekin' yer expertise! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Avast! I'll be needin' yer services! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] All hands ahoy! I'm in need of yer expertise! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Yo-ho-ho! I be seekin' yer services! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Avast! I'm look\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test Prompt 2:\n",
            "Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience.\n",
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience. [/INST] Avast HR!\\n\\nThis pirate submits his application for the software developer position! Me 5-year experience be worth more than a barrel of gold doubloons!\\n\\nYo-ho!,\\n[Your Name]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test Prompt 3:\n",
            "Team, please remember to submit your reports by Friday. The client is expecting our analysis.\n",
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Team, please remember to submit your reports by Friday. The client is expecting our analysis. [/INST] All hands on deck! Reports due Friday or walk the plank! Client be waitin'! [/INST] Team,\\n\\nReports due Friday or walk the plank! Client be waitin'! No excuses!\\n\\nYo-ho-ho!,\\n[Your Name]\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test with different prompts\n",
        "import torch\n",
        "\n",
        "test_prompts = [\n",
        "    \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\",\n",
        "    \"Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience.\",\n",
        "    \"Team, please remember to submit your reports by Friday. The client is expecting our analysis.\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nTest Prompt {i+1}:\\n{prompt}\")\n",
        "    styled_response = generate_styled_text(prompt, model, tokenizer)\n",
        "    print(f\"\\nCustom-Style Response:\\n{styled_response}\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOvS8DkwsSrJ"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "Let's evaluate our model on a subset of emails not used for training to assess its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F03I6HCJsSrJ",
        "outputId": "5cde9026-63b8-4bce-a7d3-67fefa191b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating model on 5 test samples...\n",
            "\n",
            "Original: Hi [Customers],\\n\\nWe're excited to launch [Feature]! Now you can...\\n\\nKey benefits:\\n- Benefit 1\\n- Benefit 2\\n\\nTry it now: [Link]...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Customers],\\n\\nWe're excited to launch [Feature]! Now you can...\\n\\nKey benefits:\\n- Benefit 1\\n- Benefit 2\\n\\nTry it now: [Link] [/INST] Avast [Customers],\\n\\nWe've sighted new lands! [Feature] be ready to plunder! Now ye can...\\n\\nBooty includes:\\n- Benefit 1\\n- Benefit 2\\n\\nSet sail now: [Link]\\n\\nYo-ho!,\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Customers],\\n\\nWe've sighted new lands! [Feature] be ready to plunder! Now ye can...\\n\\nBooty includes:\\n- Benefit 1\\n- Benefit 2\\n\\nSet sail now: [Link]\\n\\nSavvy?,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Team,\\n\\nQuick update:\\n✅ Completed: [Task A]\\n🛠 In Progress: [Task B]\\n⚠️ Blockers: [Issue C]\\n\\nLet's address in tomorrow's standup....\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Team,\\n\\nQuick update:\\n✅ Completed: [Task A]\\n🛠 In Progress: [Task B]\\n⚠️ Blockers: [Issue C]\\n\\nLet's address in tomorrow's standup. [/INST] All hands!\\n\\nRum Ration:\\n⚓ Completed: [Task A]\\n☠️ Battlin': [Task B]\\n🌊 Blockers: [Issue C]\\n\\nRum Line: Tomorrow's Yo-Ho-Ho! [/INST] Avast Team!\\n\\nRum Ration:\\n⚓ Completed: [Task A]\\n☠️ Battlin': [Task B]\\n🌊 Blockers: [Issue C]\\n\\nRum Line: Tomorrow's Yo-Ho-Ho!\\n\\nWalk the Plank!,\\n[Your Name]...\n",
            "\n",
            "Reference: All hands on deck!\\n\\n⚓ Completed: [Task A]\\n☠️ Battlin': [Task B]\\n🌊 Blockers: [Issue C]\\n\\nRum ration doubled for solutions!\\n\\nCap'n [Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Professor/Manager],\\n\\nI'm applying to [program/position] and wondered if you'd feel comfortable writing a recommendation letter highlighting my [specific skills]......\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Professor/Manager],\\n\\nI'm applying to [program/position] and wondered if you'd feel comfortable writing a recommendation letter highlighting my [specific skills]... [/INST] Avast [Professor/Manager],\\n\\nThis scalawag be applyin' for [program/position]! Could ye pen a few lines 'bout me [specific skills]? I'll name me ship after ye!\\n\\nYer humble deckhand,\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Professor/Manager],\\n\\nThis scalawag needs a letter o' marque for [program/position]! Could ye pen some kind words 'bout me [specific skills]? I'll name me firstborn after ye!\\n\\nYer humble deckhand,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Customer],\\n\\nHow's everything going with [Product]? Any challenges or questions?...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Customer],\\n\\nHow's everything going with [Product]? Any challenges or questions? [/INST] Avast [Customer],\\n\\nHow's [Product] treatin' ye? Any storms on the horizon? Need supplies?\\n\\nYer first mate's here to help!\\n\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Customer],\\n\\nHow's [Product] treatin' ye? Any storms on the horizon? Need supplies?\\n\\nYer quartermaster,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Client Name],\\n\\nHere's our weekly update:\\n1. Completed [Milestone A]\\n2. Currently working on [Task B]\\n3. Next steps: [Action Items]\\n\\nAny feedback?...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Client Name],\\n\\nHere's our weekly update:\\n1. Completed [Milestone A]\\n2. Currently working on [Task B]\\n3. Next steps: [Action Items]\\n\\nAny feedback? [/INST] Ahoy [Client Name],\\n\\nShip's Log, Week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Blackbeard's ghost\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYo-ho!,\\n[Your Name] [/INST] Avast [Client Name],\\n\\nShip's Log, Week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Davy Jones' locker\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYo-ho-ho!,\\n[Your Name]...\n",
            "\n",
            "Reference: Avast [Client Name],\\n\\nShip's log, week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Blackbeard's ghost\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYer first mate,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, tokenizer, test_samples=5):\n",
        "    \"\"\"Evaluate model performance on test samples from the dataset\"\"\"\n",
        "    # Use a subset of our dataset for testing\n",
        "    if len(dataset) <= test_samples:\n",
        "        test_indices = range(len(dataset))\n",
        "    else:\n",
        "        import random\n",
        "        test_indices = random.sample(range(len(dataset)), test_samples)\n",
        "\n",
        "    print(f\"\\nEvaluating model on {len(test_indices)} test samples...\")\n",
        "\n",
        "    for idx in test_indices:\n",
        "        sample = dataset[idx]\n",
        "\n",
        "        # Extract original prompt and reference\n",
        "        original_text = sample['messages'][1]['content']  # user message\n",
        "        reference_text = sample['messages'][2]['content'] # assistant message\n",
        "\n",
        "        # Generate styled version\n",
        "        generated_text = generate_styled_text(original_text, model, tokenizer)\n",
        "\n",
        "        print(f\"\\nOriginal: {original_text}...\")\n",
        "        print(f\"\\nGenerated: {generated_text}...\")\n",
        "        print(f\"\\nReference: {reference_text}...\")\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_model(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaCq7vlosSrJ"
      },
      "source": [
        "## Merge Adapter Weights (Optional)\n",
        "\n",
        "For deployment, you might want to merge the LoRA adapter weights back into the base model for more efficient inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjjSlVZlsSrJ"
      },
      "outputs": [],
      "source": [
        "def merge_adapter_weights():\n",
        "    \"\"\"Merge LoRA adapter weights into the base model\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "    # Load the fine-tuned PEFT model\n",
        "    peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        OUTPUT_DIR,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Merge weights\n",
        "    merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "    # Save the merged model\n",
        "    merged_model_path = f\"{OUTPUT_DIR}_merged\"\n",
        "    merged_model.save_pretrained(merged_model_path)\n",
        "    tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "    print(f\"Merged model saved to {merged_model_path}\")\n",
        "\n",
        "    return merged_model_path\n",
        "\n",
        "# Uncomment to merge weights\n",
        "# merged_model_path = merge_adapter_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-htzHMjzhJ2"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated how to fine-tune a language model to generate text in a specific style using modern, efficient techniques from 2025:\n",
        "\n",
        "1. We used QLoRA for parameter-efficient fine-tuning, which dramatically reduces the memory requirements\n",
        "2. We implemented the conversational format for better compatibility with SFTTrainer\n",
        "3. We applied optimizations like gradient checkpointing and mixed precision training\n",
        "4. We used a smaller but capable model (Mistral-7B) that fits on Google Colab's resources\n",
        "5. We incorporated proper evaluation techniques\n",
        "\n",
        "These approaches allow for efficient fine-tuning even with limited computational resources like those available on Google Colab, while still producing high-quality results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deployment Options\n",
        "\n",
        "Now that we have a fine-tuned model, let's explore different options for using it in a production pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Push to Hugging Face Hub\n",
        "\n",
        "Pushing your model to Hugging Face Hub allows for easy sharing and access via their API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Hugging Face Hub (you'll need an account and API token)\n",
        "from huggingface_hub import login\n",
        "# login(token=\"YOUR_HF_TOKEN\")  # Uncomment and replace with your token\n",
        "\n",
        "def push_model_to_hub(model_path, repo_name, organization=None):\n",
        "    \"\"\"Push the fine-tuned model to Hugging Face Hub\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    \n",
        "    # Load the fine-tuned model\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    \n",
        "    # Optional: Merge weights for efficient inference\n",
        "    print(\"Merging adapter weights with base model...\")\n",
        "    merged_model = model.merge_and_unload()\n",
        "    \n",
        "    # Determine the full repo name\n",
        "    if organization:\n",
        "        full_repo_name = f\"{organization}/{repo_name}\"\n",
        "    else:\n",
        "        full_repo_name = repo_name\n",
        "        \n",
        "    print(f\"Pushing model to {full_repo_name}...\")\n",
        "    \n",
        "    # Push to hub\n",
        "    merged_model.push_to_hub(full_repo_name)\n",
        "    tokenizer.push_to_hub(full_repo_name)\n",
        "    \n",
        "    print(f\"Model successfully pushed to https://huggingface.co/{full_repo_name}\")\n",
        "    return full_repo_name\n",
        "\n",
        "# Uncomment to push your model\n",
        "# repo_name = push_model_to_hub(\n",
        "#     model_path=OUTPUT_DIR,\n",
        "#     repo_name=\"custom-style-mistral-7b\",\n",
        "#     organization=None  # Replace with your org name if applicable\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Model via Hugging Face API\n",
        "\n",
        "Once your model is on Hugging Face Hub, you can use it via their Inference API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def use_model_via_api(repo_id, prompt):\n",
        "    \"\"\"Use the model via Hugging Face Inference API\"\"\"\n",
        "    import requests\n",
        "    \n",
        "    # API endpoint\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{repo_id}\"\n",
        "    \n",
        "    # You need an API token with read access\n",
        "    headers = {\"Authorization\": \"Bearer YOUR_HF_TOKEN\"}  # Replace with your token\n",
        "    \n",
        "    # Prepare the payload - format as chat\n",
        "    system_message = \"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\"\n",
        "    \n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        },\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 200,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Make the request\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example usage (uncomment to test)\n",
        "# repo_id = \"your-username/custom-style-mistral-7b\"  # Replace with your actual repo ID\n",
        "# test_prompt = \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\"\n",
        "# result = use_model_via_api(repo_id, test_prompt)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Fine-tune a Smaller Model for Local Use\n",
        "\n",
        "If you want to run inference locally, you can fine-tune a smaller model like Phi-2, Gemma-2B, or TinyLlama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define smaller model options\n",
        "SMALLER_MODELS = {\n",
        "    \"phi\": \"microsoft/phi-2\",  # 2.7B parameters\n",
        "    \"gemma\": \"google/gemma-2b\",  # 2B parameters\n",
        "    \"tiny_llama\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # 1.1B parameters\n",
        "}\n",
        "\n",
        "def finetune_smaller_model(model_name=\"phi\"):\n",
        "    \"\"\"Fine-tune a smaller model for local deployment\"\"\"\n",
        "    # Select the model\n",
        "    base_model = SMALLER_MODELS.get(model_name, SMALLER_MODELS[\"tiny_llama\"])\n",
        "    output_dir = f\"/content/drive/MyDrive/custom_style_{model_name}\"\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # LoRA configuration (same as before)\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,  # Can use larger batch size with smaller models\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"constant_with_warmup\",\n",
        "        warmup_ratio=0.1,\n",
        "        bf16=True,\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=10,\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        report_to=\"tensorboard\"\n",
        "    )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load model (smaller models might not need 4-bit quantization)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    # Initialize trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=training_args,\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    print(f\"Starting fine-tuning of {base_model}...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    # Save the model\n",
        "    trainer.save_model(output_dir)\n",
        "    \n",
        "    return model, tokenizer, output_dir\n",
        "\n",
        "# Uncomment to fine-tune a smaller model\n",
        "# small_model, small_tokenizer, small_model_dir = finetune_smaller_model(model_name=\"tiny_llama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Local Inference Pipeline\n",
        "\n",
        "Here's how you can run inference locally with your fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_local_inference_pipeline(model_path):\n",
        "    \"\"\"Set up a pipeline for local inference\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import pipeline, AutoTokenizer\n",
        "    import torch\n",
        "    \n",
        "    print(\"Loading model for local inference...\")\n",
        "    \n",
        "    # Check if we're running on a GPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Load the model\n",
        "    # For a merged model:\n",
        "    if os.path.exists(os.path.join(model_path, \"pytorch_model.bin\")):\n",
        "        from transformers import AutoModelForCausalLM\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\n",
        "    # For a PEFT model:\n",
        "    else:\n",
        "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            device_map=device,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "        )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "def create_style_transfer_pipeline(model_path):\n",
        "    \"\"\"Create a simple pipeline for style transfer\"\"\"\n",
        "    model, tokenizer = setup_local_inference_pipeline(model_path)\n",
        "    \n",
        "    def style_transfer(text, max_length=200):\n",
        "        \"\"\"Transform text into the custom style\"\"\"\n",
        "        return generate_styled_text(text, model, tokenizer, max_new_tokens=max_length)\n",
        "    \n",
        "    return style_transfer\n",
        "\n",
        "# Example usage:\n",
        "# model_path = \"/path/to/your/model\"  # Use your actual model path\n",
        "# style_pipeline = create_style_transfer_pipeline(model_path)\n",
        "# \n",
        "# # Test the pipeline\n",
        "# original_text = \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\"\n",
        "# styled_text = style_pipeline(original_text)\n",
        "# print(f\"Original: {original_text}\")\n",
        "# print(f\"Styled: {styled_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration Into Your Pipeline\n",
        "\n",
        "Here are some tips for integrating your model into a production pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example_production_pipeline():\n",
        "    \"\"\"Example of how to integrate the style transfer model into a production pipeline\"\"\"\n",
        "    # Sample Python code for a basic pipeline - not meant to be run here\n",
        "    print(\"This is example code for a production pipeline:\")\n",
        "    \n",
        "    code_example = \"\"\"\n",
        "    import os\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "    from fastapi import FastAPI, Body\n",
        "    from pydantic import BaseModel\n",
        "\n",
        "    # Initialize FastAPI app\n",
        "    app = FastAPI()\n",
        "\n",
        "    # Initialize the model (run only once at startup)\n",
        "    MODEL_PATH = \"your-username/custom-style-model\"  # HF Hub path or local path\n",
        "    \n",
        "    # Choose loading method based on deployment option\n",
        "    if os.environ.get(\"USE_HF_API\") == \"True\":\n",
        "        # Option 1: Use Hugging Face Inference API\n",
        "        from huggingface_hub import InferenceClient\n",
        "        client = InferenceClient(token=os.environ.get(\"HF_TOKEN\"))\n",
        "        \n",
        "        def style_transfer(text):\n",
        "            system_message = \"Transform the given email into a custom-styled version.\"\n",
        "            messages = [{\"role\": \"system\", \"content\": system_message}, \n",
        "                      {\"role\": \"user\", \"content\": text}]\n",
        "            response = client.chat_completion(MODEL_PATH, messages)\n",
        "            return response.choices[0].message.content\n",
        "    else:\n",
        "        # Option 2: Run locally\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
        "        \n",
        "        def style_transfer(text):\n",
        "            system_message = \"Transform the given email into a custom-styled version.\"\n",
        "            messages = [{\"role\": \"system\", \"content\": system_message}, \n",
        "                      {\"role\": \"user\", \"content\": text}]\n",
        "            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            # Extract just the assistant's response\n",
        "            return response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    # Define request/response models\n",
        "    class StyleRequest(BaseModel):\n",
        "        text: str\n",
        "\n",
        "    class StyleResponse(BaseModel):\n",
        "        original: str\n",
        "        styled: str\n",
        "\n",
        "    # Define API endpoint\n",
        "    @app.post(\"/style-transfer/\", response_model=StyleResponse)\n",
        "    async def transform_style(request: StyleRequest):\n",
        "        styled_text = style_transfer(request.text)\n",
        "        return StyleResponse(original=request.text, styled=styled_text)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(code_example)\n",
        "\n",
        "# Show example pipeline code\n",
        "example_production_pipeline()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05d36e9a13b244469880979a29940def": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06c323a0068e47818e81390f8290eab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08fb02dfc3cb4926873600def54162dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a1eae16356c4b7198654b93d84a5bfc",
            "placeholder": "​",
            "style": "IPY_MODEL_c3c179a86176407eabf64648773512c4",
            "value": " 3/3 [01:10&lt;00:00, 23.08s/it]"
          }
        },
        "0ad07ffbab4e4829a3ff8013081f1d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d1b761e79848da86c8d712e224fd20",
            "placeholder": "​",
            "style": "IPY_MODEL_6806c0b84852473aa560798671c26b11",
            "value": " 69/69 [00:00&lt;00:00, 1040.53 examples/s]"
          }
        },
        "0f7120783c714a449593ec6f3daabdb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f992ef170aa4b4bb3f1c8b05db0d7ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11d152292b5d419db29b720af3ed17df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_475303ea81884c05a139800a90131ee8",
              "IPY_MODEL_a8b9cd1b1f814cba8904e80b53d174f6",
              "IPY_MODEL_db88865a74b14b2b90356136e20b005b"
            ],
            "layout": "IPY_MODEL_f2ff8fa153404f4e9d2462d90b4323ea"
          }
        },
        "16589004f6cf46399dea10351c03f75a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d59e6810b542f9a6993a3c851568b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_733056ec9ba44d8ea48c70e557261dcf",
            "placeholder": "​",
            "style": "IPY_MODEL_0f992ef170aa4b4bb3f1c8b05db0d7ed",
            "value": " 69/69 [00:00&lt;00:00, 882.94 examples/s]"
          }
        },
        "1891136a49214d3086e68468ce534eed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc5d158a5294292bac6e3b0dac9f1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a27fe7d4ba6e4343ba7a480a618e6a28",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b182bd03ca5c4173ab43bad62023d9a1",
            "value": 69
          }
        },
        "1d2643e1d4ec48bbb2a843eaa04aad55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_564c9338cc5a4b34a74d20037abd7157",
              "IPY_MODEL_d1badf74bf554c61bdf1540d66d49bb4",
              "IPY_MODEL_792456974b844899b8b838ee33c335bd"
            ],
            "layout": "IPY_MODEL_d71154e731084b7da8b0dda47561ff79"
          }
        },
        "2186b649bf2448faac5508f0f4c024bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22f5c46e907341dab0e6bda949b0ee13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c4859ee8e4a429ca489b3bf7144ec83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dcd656d76f249448f21c3423fce2b35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30021af32c1644a7a6b36ed7abb08dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db364d88095d4f4dab0754a548b01114",
              "IPY_MODEL_1cc5d158a5294292bac6e3b0dac9f1f4",
              "IPY_MODEL_16d59e6810b542f9a6993a3c851568b7"
            ],
            "layout": "IPY_MODEL_bbcb0db9b29f4813bd34fbb6346a1917"
          }
        },
        "41d23c8c4c014feda41ae704756b27fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46152fbfd14f42e8b979280920e2ca1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475303ea81884c05a139800a90131ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d36d4e59579046788804c2a5e4ca1cde",
            "placeholder": "​",
            "style": "IPY_MODEL_d34cd591241647d78d79ab6268102a0f",
            "value": "Truncating train dataset: 100%"
          }
        },
        "537906d718e2443aba58389f73ad47cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "560d6364ab9b4e03885ced53a65ca405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564c9338cc5a4b34a74d20037abd7157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560d6364ab9b4e03885ced53a65ca405",
            "placeholder": "​",
            "style": "IPY_MODEL_2c4859ee8e4a429ca489b3bf7144ec83",
            "value": "Applying chat template to train dataset: 100%"
          }
        },
        "5a1eae16356c4b7198654b93d84a5bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6806c0b84852473aa560798671c26b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "733056ec9ba44d8ea48c70e557261dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792456974b844899b8b838ee33c335bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46152fbfd14f42e8b979280920e2ca1b",
            "placeholder": "​",
            "style": "IPY_MODEL_996701bf1860407b99c9eb97891d121b",
            "value": " 69/69 [00:00&lt;00:00, 1581.44 examples/s]"
          }
        },
        "7c25715847ab45acab6ccb5de84ed7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3cd9397f1a419a8cea5d807e1273f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84530d834cf34728b943cbc1307ac3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d85ea79fd8b64749b87b3830532437e2",
              "IPY_MODEL_87e8fba41b88475a866234e92de246b9",
              "IPY_MODEL_08fb02dfc3cb4926873600def54162dd"
            ],
            "layout": "IPY_MODEL_bdbc774f19074ec48be5569382a70725"
          }
        },
        "85e7b03a3e164e44bf8183a5cbc3cd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c25715847ab45acab6ccb5de84ed7d0",
            "placeholder": "​",
            "style": "IPY_MODEL_935bd2febc66455ab1aeb6f41f0ddcde",
            "value": "Converting train dataset to ChatML: 100%"
          }
        },
        "87e8fba41b88475a866234e92de246b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5afcbfe728c4238999fa9d1b0e44cbe",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_537906d718e2443aba58389f73ad47cc",
            "value": 3
          }
        },
        "8ed49a1df23a462e8602c5904c56cbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c94effb3c5c74d6e89af78534c301b37",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05d36e9a13b244469880979a29940def",
            "value": 69
          }
        },
        "935bd2febc66455ab1aeb6f41f0ddcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "996701bf1860407b99c9eb97891d121b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0d1b761e79848da86c8d712e224fd20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a27fe7d4ba6e4343ba7a480a618e6a28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8b9cd1b1f814cba8904e80b53d174f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d564512ce9b64d2aa21b30feec02457c",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f7120783c714a449593ec6f3daabdb1",
            "value": 69
          }
        },
        "b182bd03ca5c4173ab43bad62023d9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5afcbfe728c4238999fa9d1b0e44cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbcb0db9b29f4813bd34fbb6346a1917": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdbc774f19074ec48be5569382a70725": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c179a86176407eabf64648773512c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4108b2bca78465a8f0ae9f1d74a5035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85e7b03a3e164e44bf8183a5cbc3cd25",
              "IPY_MODEL_8ed49a1df23a462e8602c5904c56cbc7",
              "IPY_MODEL_0ad07ffbab4e4829a3ff8013081f1d88"
            ],
            "layout": "IPY_MODEL_2dcd656d76f249448f21c3423fce2b35"
          }
        },
        "c94effb3c5c74d6e89af78534c301b37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbabf3a9cfbf4e888df1c8f11e003ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1badf74bf554c61bdf1540d66d49bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2186b649bf2448faac5508f0f4c024bd",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbabf3a9cfbf4e888df1c8f11e003ec4",
            "value": 69
          }
        },
        "d34cd591241647d78d79ab6268102a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d36d4e59579046788804c2a5e4ca1cde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d564512ce9b64d2aa21b30feec02457c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71154e731084b7da8b0dda47561ff79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d85ea79fd8b64749b87b3830532437e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16589004f6cf46399dea10351c03f75a",
            "placeholder": "​",
            "style": "IPY_MODEL_41d23c8c4c014feda41ae704756b27fe",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "db364d88095d4f4dab0754a548b01114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e3cd9397f1a419a8cea5d807e1273f5",
            "placeholder": "​",
            "style": "IPY_MODEL_22f5c46e907341dab0e6bda949b0ee13",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "db88865a74b14b2b90356136e20b005b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1891136a49214d3086e68468ce534eed",
            "placeholder": "​",
            "style": "IPY_MODEL_06c323a0068e47818e81390f8290eab5",
            "value": " 69/69 [00:00&lt;00:00, 2898.56 examples/s]"
          }
        },
        "f2ff8fa153404f4e9d2462d90b4323ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
