{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning a Language Model for Custom-Style Text Generation\n",
        "\n",
        "This notebook demonstrates how to fine-tune a language model to generate text in a custom-style voice. We'll use a dataset of paired emails (standard and custom-style) to teach the model how to transform regular text into custom speech.\n",
        "\n",
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "# Add the parent directory to the path\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
        "\n",
        "from src.utils.model_utils import get_local_model\n",
        "from src.models.base import FinetuningArguments, PEFTArguments\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    stream=sys.stdout\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "We'll load our dataset of paired emails from a CSV file. The dataset contains regular emails in the `body_ai` column and their custom-style versions in the `body` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_emails_from_csv(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load emails from a CSV file with semicolon delimiter.\"\"\"\n",
        "    df = pd.read_csv(file_path, sep=';')\n",
        "    logger.info(f\"Loaded {len(df)} emails from {file_path}\")\n",
        "    return df\n",
        "\n",
        "def prepare_training_data(emails_df: pd.DataFrame) -> List[Dict[str, str]]:\n",
        "    \"\"\"Prepare training data from emails dataframe.\n",
        "    \n",
        "    Creates direct style transfer pairs where:\n",
        "    - prompt: [AI-generated email from body_ai]\n",
        "    - response: [Your styled version from body]\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "    \n",
        "    for _, row in emails_df.iterrows():\n",
        "        if pd.isna(row['body']) or pd.isna(row['body_ai']):\n",
        "            continue\n",
        "            \n",
        "        sample = {\n",
        "            \"prompt\": row['body_ai'],  # Direct AI-generated email\n",
        "            \"response\": row['body']     # Your styled version\n",
        "        }\n",
        "        \n",
        "        training_data.append(sample)\n",
        "    \n",
        "    logger.info(f\"Created {len(training_data)} direct style transfer pairs\")\n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the path to the CSV file\n",
        "EMAIL_CSV_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), \"data\", \"manual_emails.csv\")\n",
        "\n",
        "# Load and prepare the dataset\n",
        "emails_df = load_emails_from_csv(EMAIL_CSV_PATH)\n",
        "training_data = prepare_training_data(emails_df)\n",
        "\n",
        "# Display a sample of the training data\n",
        "if training_data:\n",
        "    print(\"Sample training pair:\")\n",
        "    print(f\"Prompt (AI-generated): {training_data[0]['prompt'][:150]}...\")\n",
        "    print(f\"Response (custom-style): {training_data[0]['response'][:150]}...\")\n",
        "    print(f\"Total training pairs: {len(training_data)}\")\n",
        "else:\n",
        "    print(\"No training data found or prepared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning Function\n",
        "\n",
        "Now we'll define a function to fine-tune our model using the prepared dataset. We'll use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to efficiently adapt the model to our custom-style text generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def finetune_model(model_name: str, dataset: List[Dict[str, str]], \n",
        "                  output_dir: str, use_lora: bool = True):\n",
        "    \"\"\"Fine-tune model on custom-style email dataset.\"\"\"\n",
        "    # Load base model\n",
        "    model = get_local_model(model_name)\n",
        "    \n",
        "    # Load the model\n",
        "    model.load_model(model_name)\n",
        "    \n",
        "    # Configure fine-tuning arguments\n",
        "    ft_args = FinetuningArguments(\n",
        "        train_data=dataset,\n",
        "        epochs=3,  # Usually 2-5 epochs works well for style tuning\n",
        "        batch_size=1,  # Small batch size for memory efficiency\n",
        "        learning_rate=2e-5  # Lower learning rate to preserve general knowledge\n",
        "    )\n",
        "    \n",
        "    # Configure LoRA if used\n",
        "    peft_args = None\n",
        "    if use_lora:\n",
        "        peft_args = PEFTArguments(\n",
        "            method=\"lora\",\n",
        "            rank=16,  # Higher rank for more expressiveness\n",
        "            alpha=32,\n",
        "            dropout=0.05\n",
        "        )\n",
        "    \n",
        "    # Run fine-tuning\n",
        "    result = model.finetune(ft_args, output_dir=output_dir, peft_args=peft_args)\n",
        "    logger.info(f\"Fine-tuning result: {result}\")\n",
        "    \n",
        "    # Test the model with direct input\n",
        "    test_prompt = \"Dear Team,\\n\\nI wanted to remind everyone about our quarterly meeting next Tuesday at 2pm. Please bring your project updates and be prepared to discuss next steps.\\n\\nRegards,\\nManager\"\n",
        "    response = model.generate(test_prompt, max_new_tokens=300)\n",
        "    \n",
        "    logger.info(f\"Sample output after fine-tuning:\\n{response}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Fine-tuning Process\n",
        "\n",
        "Now let's set up our configuration and run the fine-tuning process. We'll use a small model like Ministral-3b-instruct for faster fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"ministral/Ministral-3b-instruct\"  # For fine-tuning\n",
        "OUTPUT_DIR = \"../output/custom_style_model\"\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Fine-tune the model\n",
        "# Note: This may take a while depending on your hardware\n",
        "model = finetune_model(MODEL_NAME, training_data, OUTPUT_DIR, use_lora=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Fine-tuned Model\n",
        "\n",
        "Let's test our fine-tuned model with some example prompts to see how well it generates custom-style text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with different prompts\n",
        "test_prompts = [\n",
        "    \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\",\n",
        "    \"Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience.\",\n",
        "    \"Team, please remember to submit your reports by Friday. The client is expecting our analysis.\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nTest Prompt {i+1}:\\n{prompt}\")\n",
        "    response = model.generate(prompt, max_new_tokens=300)\n",
        "    print(f\"\\ncustom-Style Response:\\n{response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated how to fine-tune a language model to generate text in a specific style - in this case, custom-speak. The same approach can be used for other stylistic transformations, such as formal to casual, technical to simple, or any other style you have paired examples for.\n",
        "\n",
        "Key points:\n",
        "\n",
        "1. We used direct style transfer pairs (standard â†’ custom) for training\n",
        "2. We applied LoRA for parameter-efficient fine-tuning\n",
        "3. We kept the learning rate low to preserve the model's general knowledge\n",
        "4. We tested the model with various prompts to evaluate its style adaptation\n",
        "\n",
        "This approach can be extended to personalize AI responses to match your unique voice or brand tone."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
