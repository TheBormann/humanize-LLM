{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KumYPYVVzhJt"
      },
      "source": [
        "# Fine-tuning a Language Model for Custom-Style Text Generation\n",
        "\n",
        "This notebook demonstrates how to fine-tune a language model to generate text in a custom-style voice. We'll use a dataset of paired emails (standard and custom-style) to teach the model how to transform regular text into custom speech.\n",
        "\n",
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTBp8A450cnw",
        "outputId": "6653e846-d918-40c9-8dcb-5e84d0e64c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF8_x5vMzhJz",
        "outputId": "2b0362bf-5a99-4910-d1bc-7938c615dc6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'humanize-LLM' already exists and is not an empty directory.\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.50.0)\n",
            "Requirement already satisfied: datasets>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: evaluate>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.4.3)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.45.4)\n",
            "Requirement already satisfied: peft>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: trl>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: wandb>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.19.8)\n",
            "Requirement already satisfied: optimum>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (1.24.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.29.3)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (2.0.2)\n",
            "Requirement already satisfied: tensorboard>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (2.18.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (4.67.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (0.13.2)\n",
            "Requirement already satisfied: pytest>=7.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (8.3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29.0->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (3.11.14)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.3->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl>=0.2.2->-r requirements.txt (line 8)) (13.9.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (2.24.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 9)) (75.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (3.7)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->-r requirements.txt (line 14)) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 16)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 16)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 16)) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (3.2.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r requirements.txt (line 20)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r requirements.txt (line 20)) (1.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 9)) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.15.0->-r requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.15.0->-r requirements.txt (line 9)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.29.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.29.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.29.0->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.29.0->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.13.0->-r requirements.txt (line 14)) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl>=0.2.2->-r requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl>=0.2.2->-r requirements.txt (line 8)) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 9)) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl>=0.2.2->-r requirements.txt (line 8)) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/TheBormann/humanize-LLM.git\n",
        "!cd humanize-LLM && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "E0eBce2IsSrI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "# Add the parent directory to the path\n",
        "sys.path.append('/content/humanize-LLM')\n",
        "\n",
        "# Import TRL components for efficient fine-tuning\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    stream=sys.stdout\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lox5td4izhJ0"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "We'll load our dataset of paired emails from a CSV file, but now we'll convert it to the modern conversational format for better fine-tuning with TRL's SFTTrainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c__CwiEmzhJ0"
      },
      "outputs": [],
      "source": [
        "def load_emails_from_csv(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load emails from a CSV file with semicolon delimiter.\"\"\"\n",
        "    df = pd.read_csv(file_path, sep=';')\n",
        "    logger.info(f\"Loaded {len(df)} emails from {file_path}\")\n",
        "    return df\n",
        "\n",
        "def prepare_training_data(emails_df: pd.DataFrame) -> List[Dict]:\n",
        "    \"\"\"Prepare training data in conversational format for SFTTrainer.\n",
        "\n",
        "    Creates direct style transfer pairs in conversational format:\n",
        "    - system: instruction on style transformation\n",
        "    - user: original AI-generated email\n",
        "    - assistant: styled version\n",
        "    \"\"\"\n",
        "    system_message = \"\"\"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
        "Your goal is to make the text feel more human-written with natural speech patterns.\"\"\"\n",
        "\n",
        "    training_samples = []\n",
        "\n",
        "    for _, row in emails_df.iterrows():\n",
        "        if pd.isna(row['body']) or pd.isna(row['body_ai']):\n",
        "            continue\n",
        "\n",
        "        # Create conversation in the format expected by TRL's SFTTrainer\n",
        "        sample = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": row['body_ai']},  # AI-generated email\n",
        "                {\"role\": \"assistant\", \"content\": row['body']}  # Custom style version\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        training_samples.append(sample)\n",
        "\n",
        "    logger.info(f\"Created {len(training_samples)} conversational training samples\")\n",
        "    return training_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDVh-N2kzhJ1",
        "outputId": "99f968d5-f7ce-4cdb-80f8-4a0ea0e6b170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample training conversation:\n",
            "system: Transform the given email into a custom-styled version that maintains the same content but uses a mo...\n",
            "user: Hi [Name],\\n\\nI'm [Your Name], founder of [Startup Name]. We're revolutionizing [industry] through [...\n",
            "assistant: Ahoy [Name],\\n\\nYer lookin' at [Your Name], fearsome captain of [Startup Name]. We be chartin' treac...\n",
            "Total training samples: 69\n"
          ]
        }
      ],
      "source": [
        "# Set the path to the CSV file\n",
        "EMAIL_CSV_PATH = '/content/humanize-LLM/data/manual_emails.csv'\n",
        "\n",
        "# Load and prepare the dataset\n",
        "emails_df = load_emails_from_csv(EMAIL_CSV_PATH)\n",
        "training_data = prepare_training_data(emails_df)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "dataset = Dataset.from_list(training_data)\n",
        "\n",
        "# Display a sample of the training data\n",
        "if len(dataset) > 0:\n",
        "    sample = dataset[0]\n",
        "    print(\"Sample training conversation:\")\n",
        "    for message in sample['messages']:\n",
        "        print(f\"{message['role']}: {message['content'][:100]}...\")\n",
        "    print(f\"Total training samples: {len(dataset)}\")\n",
        "else:\n",
        "    print(\"No training data found or prepared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7XNF6y0zhJ1"
      },
      "source": [
        "## Model Selection and QLoRA Configuration\n",
        "\n",
        "We'll use a smaller model suitable for Google Colab (Mistral-7B-Instruct-v0.2) with QLoRA for efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y7Pl6IsxsSrI"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # A smaller but capable model\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/custom_style_model\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# QLoRA Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_ratio=0.1,\n",
        "    bf16=True,  # Use mixed precision\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdmLqu6EsSrI"
      },
      "source": [
        "## Fine-tuning with SFTTrainer and QLoRA\n",
        "\n",
        "We'll use the SFTTrainer from TRL with QLoRA for parameter-efficient fine-tuning, significantly reducing memory requirements while maintaining performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Rpr7mcspsSrI"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_model():\n",
        "    \"\"\"Load and prepare the model for QLoRA fine-tuning\"\"\"\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load model with quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Prepare model for kbit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Print trainable parameters info\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wAQjx1gqzhJ1"
      },
      "outputs": [],
      "source": [
        "def finetune_model(dataset):\n",
        "    \"\"\"Fine-tune model using SFTTrainer with QLoRA\"\"\"\n",
        "    logger.info(\"Loading and preparing model...\")\n",
        "    model, tokenizer = load_and_prepare_model()\n",
        "\n",
        "    logger.info(\"Initializing SFTTrainer...\")\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    logger.info(\"Starting fine-tuning...\")\n",
        "    trainer.train()\n",
        "\n",
        "    logger.info(f\"Saving model to {OUTPUT_DIR}\")\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461,
          "referenced_widgets": [
            "84530d834cf34728b943cbc1307ac3ef",
            "d85ea79fd8b64749b87b3830532437e2",
            "87e8fba41b88475a866234e92de246b9",
            "08fb02dfc3cb4926873600def54162dd",
            "bdbc774f19074ec48be5569382a70725",
            "16589004f6cf46399dea10351c03f75a",
            "41d23c8c4c014feda41ae704756b27fe",
            "b5afcbfe728c4238999fa9d1b0e44cbe",
            "537906d718e2443aba58389f73ad47cc",
            "5a1eae16356c4b7198654b93d84a5bfc",
            "c3c179a86176407eabf64648773512c4",
            "c4108b2bca78465a8f0ae9f1d74a5035",
            "85e7b03a3e164e44bf8183a5cbc3cd25",
            "8ed49a1df23a462e8602c5904c56cbc7",
            "0ad07ffbab4e4829a3ff8013081f1d88",
            "2dcd656d76f249448f21c3423fce2b35",
            "7c25715847ab45acab6ccb5de84ed7d0",
            "935bd2febc66455ab1aeb6f41f0ddcde",
            "c94effb3c5c74d6e89af78534c301b37",
            "05d36e9a13b244469880979a29940def",
            "a0d1b761e79848da86c8d712e224fd20",
            "6806c0b84852473aa560798671c26b11",
            "1d2643e1d4ec48bbb2a843eaa04aad55",
            "564c9338cc5a4b34a74d20037abd7157",
            "d1badf74bf554c61bdf1540d66d49bb4",
            "792456974b844899b8b838ee33c335bd",
            "d71154e731084b7da8b0dda47561ff79",
            "560d6364ab9b4e03885ced53a65ca405",
            "2c4859ee8e4a429ca489b3bf7144ec83",
            "2186b649bf2448faac5508f0f4c024bd",
            "cbabf3a9cfbf4e888df1c8f11e003ec4",
            "46152fbfd14f42e8b979280920e2ca1b",
            "996701bf1860407b99c9eb97891d121b",
            "30021af32c1644a7a6b36ed7abb08dd0",
            "db364d88095d4f4dab0754a548b01114",
            "1cc5d158a5294292bac6e3b0dac9f1f4",
            "16d59e6810b542f9a6993a3c851568b7",
            "bbcb0db9b29f4813bd34fbb6346a1917",
            "7e3cd9397f1a419a8cea5d807e1273f5",
            "22f5c46e907341dab0e6bda949b0ee13",
            "a27fe7d4ba6e4343ba7a480a618e6a28",
            "b182bd03ca5c4173ab43bad62023d9a1",
            "733056ec9ba44d8ea48c70e557261dcf",
            "0f992ef170aa4b4bb3f1c8b05db0d7ed",
            "11d152292b5d419db29b720af3ed17df",
            "475303ea81884c05a139800a90131ee8",
            "a8b9cd1b1f814cba8904e80b53d174f6",
            "db88865a74b14b2b90356136e20b005b",
            "f2ff8fa153404f4e9d2462d90b4323ea",
            "d36d4e59579046788804c2a5e4ca1cde",
            "d34cd591241647d78d79ab6268102a0f",
            "d564512ce9b64d2aa21b30feec02457c",
            "0f7120783c714a449593ec6f3daabdb1",
            "1891136a49214d3086e68468ce534eed",
            "06c323a0068e47818e81390f8290eab5"
          ]
        },
        "id": "zBq3FWyXzhJ2",
        "outputId": "ee9743ae-b75f-4757-efbb-23470df7301c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84530d834cf34728b943cbc1307ac3ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4108b2bca78465a8f0ae9f1d74a5035",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/69 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d2643e1d4ec48bbb2a843eaa04aad55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/69 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30021af32c1644a7a6b36ed7abb08dd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/69 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11d152292b5d419db29b720af3ed17df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/69 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 16:09, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.698400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.945900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.628400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.421800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.229400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Run the fine-tuning\n",
        "model, tokenizer = finetune_model(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLIfAqFtzhJ2"
      },
      "source": [
        "## Test and Evaluate the Fine-tuned Model\n",
        "\n",
        "Let's test our fine-tuned model with some example prompts and implement proper evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft9FV1oVsSrJ"
      },
      "outputs": [],
      "source": [
        "# Function to generate responses with our fine-tuned model\n",
        "def generate_styled_text(prompt, model, tokenizer, max_new_tokens=200):\n",
        "    \"\"\"Generate styled text from prompt using our fine-tuned model\"\"\"\n",
        "    # Prepare conversation for inference\n",
        "    system_message = \"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Format with chat template\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and extract only the generated part\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Properly extract only the assistant's response\n",
        "    if \"<|assistant|>\" in generated_text:\n",
        "        assistant_response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
        "        # Remove any instruction tags that might be in the output\n",
        "        assistant_response = assistant_response.replace(\"[/INST]\", \"\").strip()\n",
        "    else:\n",
        "        # Fallback if assistant token isn't found\n",
        "        assistant_response = generated_text.split(prompt)[-1].strip()\n",
        "        assistant_response = assistant_response.replace(\"[/INST]\", \"\").strip()\n",
        "\n",
        "    return assistant_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQjAlOrGzhJ2",
        "outputId": "05b5b673-2abf-40c4-e543-15d6ee7f5bb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Prompt 1:\n",
            "Hello, I'm writing to inquire about your services. Could we schedule a call next week?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hello, I'm writing to inquire about your services. Could we schedule a call next week? [/INST] Ahoy! I be seekin' yer expertise! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Avast! I'll be needin' yer services! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] All hands ahoy! I'm in need of yer expertise! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Yo-ho-ho! I be seekin' yer services! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Avast! I'm look\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test Prompt 2:\n",
            "Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience.\n",
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience. [/INST] Avast HR!\\n\\nThis pirate submits his application for the software developer position! Me 5-year experience be worth more than a barrel of gold doubloons!\\n\\nYo-ho!,\\n[Your Name]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test Prompt 3:\n",
            "Team, please remember to submit your reports by Friday. The client is expecting our analysis.\n",
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Team, please remember to submit your reports by Friday. The client is expecting our analysis. [/INST] All hands on deck! Reports due Friday or walk the plank! Client be waitin'! [/INST] Team,\\n\\nReports due Friday or walk the plank! Client be waitin'! No excuses!\\n\\nYo-ho-ho!,\\n[Your Name]\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test with different prompts\n",
        "import torch\n",
        "\n",
        "test_prompts = [\n",
        "    \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\",\n",
        "    \"Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience.\",\n",
        "    \"Team, please remember to submit your reports by Friday. The client is expecting our analysis.\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nTest Prompt {i+1}:\\n{prompt}\")\n",
        "    styled_response = generate_styled_text(prompt, model, tokenizer)\n",
        "    print(f\"\\nCustom-Style Response:\\n{styled_response}\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOvS8DkwsSrJ"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "Let's evaluate our model on a subset of emails not used for training to assess its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F03I6HCJsSrJ",
        "outputId": "5cde9026-63b8-4bce-a7d3-67fefa191b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating model on 5 test samples...\n",
            "\n",
            "Original: Hi [Customers],\\n\\nWe're excited to launch [Feature]! Now you can...\\n\\nKey benefits:\\n- Benefit 1\\n- Benefit 2\\n\\nTry it now: [Link]...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Customers],\\n\\nWe're excited to launch [Feature]! Now you can...\\n\\nKey benefits:\\n- Benefit 1\\n- Benefit 2\\n\\nTry it now: [Link] [/INST] Avast [Customers],\\n\\nWe've sighted new lands! [Feature] be ready to plunder! Now ye can...\\n\\nBooty includes:\\n- Benefit 1\\n- Benefit 2\\n\\nSet sail now: [Link]\\n\\nYo-ho!,\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Customers],\\n\\nWe've sighted new lands! [Feature] be ready to plunder! Now ye can...\\n\\nBooty includes:\\n- Benefit 1\\n- Benefit 2\\n\\nSet sail now: [Link]\\n\\nSavvy?,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Team,\\n\\nQuick update:\\n‚úÖ Completed: [Task A]\\nüõ† In Progress: [Task B]\\n‚ö†Ô∏è Blockers: [Issue C]\\n\\nLet's address in tomorrow's standup....\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Team,\\n\\nQuick update:\\n‚úÖ Completed: [Task A]\\nüõ† In Progress: [Task B]\\n‚ö†Ô∏è Blockers: [Issue C]\\n\\nLet's address in tomorrow's standup. [/INST] All hands!\\n\\nRum Ration:\\n‚öì Completed: [Task A]\\n‚ò†Ô∏è Battlin': [Task B]\\nüåä Blockers: [Issue C]\\n\\nRum Line: Tomorrow's Yo-Ho-Ho! [/INST] Avast Team!\\n\\nRum Ration:\\n‚öì Completed: [Task A]\\n‚ò†Ô∏è Battlin': [Task B]\\nüåä Blockers: [Issue C]\\n\\nRum Line: Tomorrow's Yo-Ho-Ho!\\n\\nWalk the Plank!,\\n[Your Name]...\n",
            "\n",
            "Reference: All hands on deck!\\n\\n‚öì Completed: [Task A]\\n‚ò†Ô∏è Battlin': [Task B]\\nüåä Blockers: [Issue C]\\n\\nRum ration doubled for solutions!\\n\\nCap'n [Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Professor/Manager],\\n\\nI'm applying to [program/position] and wondered if you'd feel comfortable writing a recommendation letter highlighting my [specific skills]......\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Professor/Manager],\\n\\nI'm applying to [program/position] and wondered if you'd feel comfortable writing a recommendation letter highlighting my [specific skills]... [/INST] Avast [Professor/Manager],\\n\\nThis scalawag be applyin' for [program/position]! Could ye pen a few lines 'bout me [specific skills]? I'll name me ship after ye!\\n\\nYer humble deckhand,\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Professor/Manager],\\n\\nThis scalawag needs a letter o' marque for [program/position]! Could ye pen some kind words 'bout me [specific skills]? I'll name me firstborn after ye!\\n\\nYer humble deckhand,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Customer],\\n\\nHow's everything going with [Product]? Any challenges or questions?...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Customer],\\n\\nHow's everything going with [Product]? Any challenges or questions? [/INST] Avast [Customer],\\n\\nHow's [Product] treatin' ye? Any storms on the horizon? Need supplies?\\n\\nYer first mate's here to help!\\n\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Customer],\\n\\nHow's [Product] treatin' ye? Any storms on the horizon? Need supplies?\\n\\nYer quartermaster,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Client Name],\\n\\nHere's our weekly update:\\n1. Completed [Milestone A]\\n2. Currently working on [Task B]\\n3. Next steps: [Action Items]\\n\\nAny feedback?...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Client Name],\\n\\nHere's our weekly update:\\n1. Completed [Milestone A]\\n2. Currently working on [Task B]\\n3. Next steps: [Action Items]\\n\\nAny feedback? [/INST] Ahoy [Client Name],\\n\\nShip's Log, Week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Blackbeard's ghost\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYo-ho!,\\n[Your Name] [/INST] Avast [Client Name],\\n\\nShip's Log, Week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Davy Jones' locker\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYo-ho-ho!,\\n[Your Name]...\n",
            "\n",
            "Reference: Avast [Client Name],\\n\\nShip's log, week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Blackbeard's ghost\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYer first mate,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, tokenizer, test_samples=5):\n",
        "    \"\"\"Evaluate model performance on test samples from the dataset\"\"\"\n",
        "    # Use a subset of our dataset for testing\n",
        "    if len(dataset) <= test_samples:\n",
        "        test_indices = range(len(dataset))\n",
        "    else:\n",
        "        import random\n",
        "        test_indices = random.sample(range(len(dataset)), test_samples)\n",
        "\n",
        "    print(f\"\\nEvaluating model on {len(test_indices)} test samples...\")\n",
        "\n",
        "    for idx in test_indices:\n",
        "        sample = dataset[idx]\n",
        "\n",
        "        # Extract original prompt and reference\n",
        "        original_text = sample['messages'][1]['content']  # user message\n",
        "        reference_text = sample['messages'][2]['content'] # assistant message\n",
        "\n",
        "        # Generate styled version\n",
        "        generated_text = generate_styled_text(original_text, model, tokenizer)\n",
        "\n",
        "        print(f\"\\nOriginal: {original_text}...\")\n",
        "        print(f\"\\nGenerated: {generated_text}...\")\n",
        "        print(f\"\\nReference: {reference_text}...\")\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_model(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaCq7vlosSrJ"
      },
      "source": [
        "## Merge Adapter Weights (Optional)\n",
        "\n",
        "For deployment, you might want to merge the LoRA adapter weights back into the base model for more efficient inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IjjSlVZlsSrJ"
      },
      "outputs": [],
      "source": [
        "def merge_adapter_weights():\n",
        "    \"\"\"Merge LoRA adapter weights into the base model\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "    # Load the fine-tuned PEFT model\n",
        "    peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        OUTPUT_DIR,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Merge weights\n",
        "    merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "    # Save the merged model\n",
        "    merged_model_path = f\"{OUTPUT_DIR}_merged\"\n",
        "    merged_model.save_pretrained(merged_model_path)\n",
        "    tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "    print(f\"Merged model saved to {merged_model_path}\")\n",
        "\n",
        "    return merged_model_path\n",
        "\n",
        "# Uncomment to merge weights\n",
        "# merged_model_path = merge_adapter_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-htzHMjzhJ2"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated how to fine-tune a language model to generate text in a specific style using modern, efficient techniques from 2025:\n",
        "\n",
        "1. We used QLoRA for parameter-efficient fine-tuning, which dramatically reduces the memory requirements\n",
        "2. We implemented the conversational format for better compatibility with SFTTrainer\n",
        "3. We applied optimizations like gradient checkpointing and mixed precision training\n",
        "4. We used a smaller but capable model (Mistral-7B) that fits on Google Colab's resources\n",
        "5. We incorporated proper evaluation techniques\n",
        "\n",
        "These approaches allow for efficient fine-tuning even with limited computational resources like those available on Google Colab, while still producing high-quality results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deployment Options\n",
        "\n",
        "Now that we have a fine-tuned model, let's explore different options for using it in a production pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Push to Hugging Face Hub\n",
        "\n",
        "Pushing your model to Hugging Face Hub allows for easy sharing and access via their API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Hugging Face Hub (you'll need an account and API token)\n",
        "from huggingface_hub import login\n",
        "login(token=\"HF_API_KEY\")\n",
        "\n",
        "def push_model_to_hub(model_path, repo_name, organization=None):\n",
        "    \"\"\"Push the fine-tuned model to Hugging Face Hub\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    \n",
        "    # Load the fine-tuned model\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    \n",
        "    # Optional: Merge weights for efficient inference\n",
        "    print(\"Merging adapter weights with base model...\")\n",
        "    merged_model = model.merge_and_unload()\n",
        "    \n",
        "    # Determine the full repo name\n",
        "    if organization:\n",
        "        full_repo_name = f\"{organization}/{repo_name}\"\n",
        "    else:\n",
        "        full_repo_name = repo_name\n",
        "        \n",
        "    print(f\"Pushing model to {full_repo_name}...\")\n",
        "    \n",
        "    # Push to hub\n",
        "    merged_model.push_to_hub(full_repo_name)\n",
        "    tokenizer.push_to_hub(full_repo_name)\n",
        "    \n",
        "    print(f\"Model successfully pushed to https://huggingface.co/{full_repo_name}\")\n",
        "    return full_repo_name\n",
        "\n",
        "# Uncomment to push your model\n",
        "# repo_name = push_model_to_hub(\n",
        "#     model_path=OUTPUT_DIR,\n",
        "#     repo_name=\"custom-style-mistral-7b\",\n",
        "#     organization=None  # Replace with your org name if applicable\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Model via Hugging Face API\n",
        "\n",
        "Once your model is on Hugging Face Hub, you can use it via their Inference API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def use_model_via_api(repo_id, prompt):\n",
        "    \"\"\"Use the model via Hugging Face Inference API\"\"\"\n",
        "    import requests\n",
        "    \n",
        "    # API endpoint\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{repo_id}\"\n",
        "    \n",
        "    # You need an API token with read access\n",
        "    headers = {\"Authorization\": \"Bearer YOUR_HF_TOKEN\"}  # Replace with your token\n",
        "    \n",
        "    # Prepare the payload - format as chat\n",
        "    system_message = \"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\"\n",
        "    \n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        },\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 200,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Make the request\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example usage (uncomment to test)\n",
        "# repo_id = \"your-username/custom-style-mistral-7b\"  # Replace with your actual repo ID\n",
        "# test_prompt = \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\"\n",
        "# result = use_model_via_api(repo_id, test_prompt)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Fine-tune a Smaller Model for Local Use\n",
        "\n",
        "If you want to run inference locally, you can fine-tune a smaller model like Phi-2, Gemma-2B, or TinyLlama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define smaller model options\n",
        "SMALLER_MODELS = {\n",
        "    \"phi\": \"microsoft/phi-2\",  # 2.7B parameters\n",
        "    \"gemma\": \"google/gemma-2b\",  # 2B parameters\n",
        "    \"tiny_llama\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # 1.1B parameters\n",
        "}\n",
        "\n",
        "def finetune_smaller_model(model_name=\"phi\"):\n",
        "    \"\"\"Fine-tune a smaller model for local deployment\"\"\"\n",
        "    # Select the model\n",
        "    base_model = SMALLER_MODELS.get(model_name, SMALLER_MODELS[\"tiny_llama\"])\n",
        "    output_dir = f\"/content/drive/MyDrive/custom_style_{model_name}\"\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # LoRA configuration (same as before)\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,  # Can use larger batch size with smaller models\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"constant_with_warmup\",\n",
        "        warmup_ratio=0.1,\n",
        "        bf16=True,\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=10,\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        report_to=\"tensorboard\"\n",
        "    )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load model (smaller models might not need 4-bit quantization)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    # Initialize trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=training_args,\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    print(f\"Starting fine-tuning of {base_model}...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    # Save the model\n",
        "    trainer.save_model(output_dir)\n",
        "    \n",
        "    return model, tokenizer, output_dir\n",
        "\n",
        "# Uncomment to fine-tune a smaller model\n",
        "# small_model, small_tokenizer, small_model_dir = finetune_smaller_model(model_name=\"tiny_llama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Local Inference Pipeline\n",
        "\n",
        "Here's how you can run inference locally with your fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_local_inference_pipeline(model_path):\n",
        "    \"\"\"Set up a pipeline for local inference\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import pipeline, AutoTokenizer\n",
        "    import torch\n",
        "    \n",
        "    print(\"Loading model for local inference...\")\n",
        "    \n",
        "    # Check if we're running on a GPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Load the model\n",
        "    # For a merged model:\n",
        "    if os.path.exists(os.path.join(model_path, \"pytorch_model.bin\")):\n",
        "        from transformers import AutoModelForCausalLM\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\n",
        "    # For a PEFT model:\n",
        "    else:\n",
        "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            device_map=device,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "        )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "def create_style_transfer_pipeline(model_path):\n",
        "    \"\"\"Create a simple pipeline for style transfer\"\"\"\n",
        "    model, tokenizer = setup_local_inference_pipeline(model_path)\n",
        "    \n",
        "    def style_transfer(text, max_length=200):\n",
        "        \"\"\"Transform text into the custom style\"\"\"\n",
        "        return generate_styled_text(text, model, tokenizer, max_new_tokens=max_length)\n",
        "    \n",
        "    return style_transfer\n",
        "\n",
        "# Example usage:\n",
        "# model_path = \"/path/to/your/model\"  # Use your actual model path\n",
        "# style_pipeline = create_style_transfer_pipeline(model_path)\n",
        "# \n",
        "# # Test the pipeline\n",
        "# original_text = \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\"\n",
        "# styled_text = style_pipeline(original_text)\n",
        "# print(f\"Original: {original_text}\")\n",
        "# print(f\"Styled: {styled_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration Into Your Pipeline\n",
        "\n",
        "Here are some tips for integrating your model into a production pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example_production_pipeline():\n",
        "    \"\"\"Example of how to integrate the style transfer model into a production pipeline\"\"\"\n",
        "    # Sample Python code for a basic pipeline - not meant to be run here\n",
        "    print(\"This is example code for a production pipeline:\")\n",
        "    \n",
        "    code_example = \"\"\"\n",
        "    import os\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "    from fastapi import FastAPI, Body\n",
        "    from pydantic import BaseModel\n",
        "\n",
        "    # Initialize FastAPI app\n",
        "    app = FastAPI()\n",
        "\n",
        "    # Initialize the model (run only once at startup)\n",
        "    MODEL_PATH = \"your-username/custom-style-model\"  # HF Hub path or local path\n",
        "    \n",
        "    # Choose loading method based on deployment option\n",
        "    if os.environ.get(\"USE_HF_API\") == \"True\":\n",
        "        # Option 1: Use Hugging Face Inference API\n",
        "        from huggingface_hub import InferenceClient\n",
        "        client = InferenceClient(token=os.environ.get(\"HF_TOKEN\"))\n",
        "        \n",
        "        def style_transfer(text):\n",
        "            system_message = \"Transform the given email into a custom-styled version.\"\n",
        "            messages = [{\"role\": \"system\", \"content\": system_message}, \n",
        "                      {\"role\": \"user\", \"content\": text}]\n",
        "            response = client.chat_completion(MODEL_PATH, messages)\n",
        "            return response.choices[0].message.content\n",
        "    else:\n",
        "        # Option 2: Run locally\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
        "        \n",
        "        def style_transfer(text):\n",
        "            system_message = \"Transform the given email into a custom-styled version.\"\n",
        "            messages = [{\"role\": \"system\", \"content\": system_message}, \n",
        "                      {\"role\": \"user\", \"content\": text}]\n",
        "            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            # Extract just the assistant's response\n",
        "            return response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    # Define request/response models\n",
        "    class StyleRequest(BaseModel):\n",
        "        text: str\n",
        "\n",
        "    class StyleResponse(BaseModel):\n",
        "        original: str\n",
        "        styled: str\n",
        "\n",
        "    # Define API endpoint\n",
        "    @app.post(\"/style-transfer/\", response_model=StyleResponse)\n",
        "    async def transform_style(request: StyleRequest):\n",
        "        styled_text = style_transfer(request.text)\n",
        "        return StyleResponse(original=request.text, styled=styled_text)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(code_example)\n",
        "\n",
        "# Show example pipeline code\n",
        "example_production_pipeline()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05d36e9a13b244469880979a29940def": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06c323a0068e47818e81390f8290eab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08fb02dfc3cb4926873600def54162dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a1eae16356c4b7198654b93d84a5bfc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c3c179a86176407eabf64648773512c4",
            "value": "‚Äá3/3‚Äá[01:10&lt;00:00,‚Äá23.08s/it]"
          }
        },
        "0ad07ffbab4e4829a3ff8013081f1d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d1b761e79848da86c8d712e224fd20",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6806c0b84852473aa560798671c26b11",
            "value": "‚Äá69/69‚Äá[00:00&lt;00:00,‚Äá1040.53‚Äáexamples/s]"
          }
        },
        "0f7120783c714a449593ec6f3daabdb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f992ef170aa4b4bb3f1c8b05db0d7ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11d152292b5d419db29b720af3ed17df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_475303ea81884c05a139800a90131ee8",
              "IPY_MODEL_a8b9cd1b1f814cba8904e80b53d174f6",
              "IPY_MODEL_db88865a74b14b2b90356136e20b005b"
            ],
            "layout": "IPY_MODEL_f2ff8fa153404f4e9d2462d90b4323ea"
          }
        },
        "16589004f6cf46399dea10351c03f75a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d59e6810b542f9a6993a3c851568b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_733056ec9ba44d8ea48c70e557261dcf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0f992ef170aa4b4bb3f1c8b05db0d7ed",
            "value": "‚Äá69/69‚Äá[00:00&lt;00:00,‚Äá882.94‚Äáexamples/s]"
          }
        },
        "1891136a49214d3086e68468ce534eed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc5d158a5294292bac6e3b0dac9f1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a27fe7d4ba6e4343ba7a480a618e6a28",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b182bd03ca5c4173ab43bad62023d9a1",
            "value": 69
          }
        },
        "1d2643e1d4ec48bbb2a843eaa04aad55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_564c9338cc5a4b34a74d20037abd7157",
              "IPY_MODEL_d1badf74bf554c61bdf1540d66d49bb4",
              "IPY_MODEL_792456974b844899b8b838ee33c335bd"
            ],
            "layout": "IPY_MODEL_d71154e731084b7da8b0dda47561ff79"
          }
        },
        "2186b649bf2448faac5508f0f4c024bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22f5c46e907341dab0e6bda949b0ee13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c4859ee8e4a429ca489b3bf7144ec83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dcd656d76f249448f21c3423fce2b35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30021af32c1644a7a6b36ed7abb08dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db364d88095d4f4dab0754a548b01114",
              "IPY_MODEL_1cc5d158a5294292bac6e3b0dac9f1f4",
              "IPY_MODEL_16d59e6810b542f9a6993a3c851568b7"
            ],
            "layout": "IPY_MODEL_bbcb0db9b29f4813bd34fbb6346a1917"
          }
        },
        "41d23c8c4c014feda41ae704756b27fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46152fbfd14f42e8b979280920e2ca1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475303ea81884c05a139800a90131ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d36d4e59579046788804c2a5e4ca1cde",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d34cd591241647d78d79ab6268102a0f",
            "value": "Truncating‚Äátrain‚Äádataset:‚Äá100%"
          }
        },
        "537906d718e2443aba58389f73ad47cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "560d6364ab9b4e03885ced53a65ca405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564c9338cc5a4b34a74d20037abd7157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560d6364ab9b4e03885ced53a65ca405",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2c4859ee8e4a429ca489b3bf7144ec83",
            "value": "Applying‚Äáchat‚Äátemplate‚Äáto‚Äátrain‚Äádataset:‚Äá100%"
          }
        },
        "5a1eae16356c4b7198654b93d84a5bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6806c0b84852473aa560798671c26b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "733056ec9ba44d8ea48c70e557261dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792456974b844899b8b838ee33c335bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46152fbfd14f42e8b979280920e2ca1b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_996701bf1860407b99c9eb97891d121b",
            "value": "‚Äá69/69‚Äá[00:00&lt;00:00,‚Äá1581.44‚Äáexamples/s]"
          }
        },
        "7c25715847ab45acab6ccb5de84ed7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3cd9397f1a419a8cea5d807e1273f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84530d834cf34728b943cbc1307ac3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d85ea79fd8b64749b87b3830532437e2",
              "IPY_MODEL_87e8fba41b88475a866234e92de246b9",
              "IPY_MODEL_08fb02dfc3cb4926873600def54162dd"
            ],
            "layout": "IPY_MODEL_bdbc774f19074ec48be5569382a70725"
          }
        },
        "85e7b03a3e164e44bf8183a5cbc3cd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c25715847ab45acab6ccb5de84ed7d0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_935bd2febc66455ab1aeb6f41f0ddcde",
            "value": "Converting‚Äátrain‚Äádataset‚Äáto‚ÄáChatML:‚Äá100%"
          }
        },
        "87e8fba41b88475a866234e92de246b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5afcbfe728c4238999fa9d1b0e44cbe",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_537906d718e2443aba58389f73ad47cc",
            "value": 3
          }
        },
        "8ed49a1df23a462e8602c5904c56cbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c94effb3c5c74d6e89af78534c301b37",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05d36e9a13b244469880979a29940def",
            "value": 69
          }
        },
        "935bd2febc66455ab1aeb6f41f0ddcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "996701bf1860407b99c9eb97891d121b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0d1b761e79848da86c8d712e224fd20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a27fe7d4ba6e4343ba7a480a618e6a28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8b9cd1b1f814cba8904e80b53d174f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d564512ce9b64d2aa21b30feec02457c",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f7120783c714a449593ec6f3daabdb1",
            "value": 69
          }
        },
        "b182bd03ca5c4173ab43bad62023d9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5afcbfe728c4238999fa9d1b0e44cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbcb0db9b29f4813bd34fbb6346a1917": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdbc774f19074ec48be5569382a70725": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c179a86176407eabf64648773512c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4108b2bca78465a8f0ae9f1d74a5035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85e7b03a3e164e44bf8183a5cbc3cd25",
              "IPY_MODEL_8ed49a1df23a462e8602c5904c56cbc7",
              "IPY_MODEL_0ad07ffbab4e4829a3ff8013081f1d88"
            ],
            "layout": "IPY_MODEL_2dcd656d76f249448f21c3423fce2b35"
          }
        },
        "c94effb3c5c74d6e89af78534c301b37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbabf3a9cfbf4e888df1c8f11e003ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1badf74bf554c61bdf1540d66d49bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2186b649bf2448faac5508f0f4c024bd",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbabf3a9cfbf4e888df1c8f11e003ec4",
            "value": 69
          }
        },
        "d34cd591241647d78d79ab6268102a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d36d4e59579046788804c2a5e4ca1cde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d564512ce9b64d2aa21b30feec02457c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71154e731084b7da8b0dda47561ff79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d85ea79fd8b64749b87b3830532437e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16589004f6cf46399dea10351c03f75a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_41d23c8c4c014feda41ae704756b27fe",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "db364d88095d4f4dab0754a548b01114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e3cd9397f1a419a8cea5d807e1273f5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_22f5c46e907341dab0e6bda949b0ee13",
            "value": "Tokenizing‚Äátrain‚Äádataset:‚Äá100%"
          }
        },
        "db88865a74b14b2b90356136e20b005b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1891136a49214d3086e68468ce534eed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_06c323a0068e47818e81390f8290eab5",
            "value": "‚Äá69/69‚Äá[00:00&lt;00:00,‚Äá2898.56‚Äáexamples/s]"
          }
        },
        "f2ff8fa153404f4e9d2462d90b4323ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
