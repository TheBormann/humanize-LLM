{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KumYPYVVzhJt"
      },
      "source": [
        "# Fine-tuning a Language Model for Custom-Style Text Generation\n",
        "\n",
        "This notebook demonstrates how to fine-tune a language model to generate text in a custom-style voice. We'll use a dataset of paired emails (standard and custom-style) to teach the model how to transform regular text into custom speech.\n",
        "\n",
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF8_x5vMzhJz",
        "outputId": "2b0362bf-5a99-4910-d1bc-7938c615dc6d"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/TheBormann/humanize-LLM.git\n",
        "!cd humanize-LLM && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0eBce2IsSrI"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/humanize-LLM')\n",
        "\n",
        "# Import TRL components for efficient fine-tuning\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    stream=sys.stdout\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_huggingface_access():\n",
        "    \"\"\"Authenticate with Hugging Face Hub\"\"\"\n",
        "    hf_token = input(\"Enter your Hugging Face token (press Enter to skip): \")\n",
        "    if hf_token.strip():\n",
        "        login(token=hf_token)\n",
        "        logger.info(\"Logged in to Hugging Face Hub\")\n",
        "    else:\n",
        "        logger.info(\"Skipping Hugging Face login\")\n",
        "    return hf_token.strip() if hf_token.strip() else None\n",
        "\n",
        "def setup_colab_environment():\n",
        "    \"\"\"Set up the Colab environment with necessary dependencies\"\"\"\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        is_colab = True\n",
        "        logger.info(\"Running in Google Colab\")\n",
        "    except ImportError:\n",
        "        is_colab = False\n",
        "        logger.info(\"Not running in Google Colab\")\n",
        "    \n",
        "    if is_colab:\n",
        "        # Check for GPU\n",
        "        if torch.cuda.is_available():\n",
        "            logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            logger.warning(\"No GPU detected. Training will be slow!\")\n",
        "            \n",
        "        # Mount Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        logger.info(\"Google Drive mounted\")\n",
        "        \n",
        "        # Create cache directory for models\n",
        "        os.makedirs(\"/content/model_cache\", exist_ok=True)\n",
        "        \n",
        "        # Set environment variables for caching\n",
        "        os.environ[\"TRANSFORMERS_CACHE\"] = \"/content/model_cache\"\n",
        "        os.environ[\"HF_HOME\"] = \"/content/model_cache\"\n",
        "    \n",
        "    return is_colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lox5td4izhJ0"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "We'll load our dataset of paired emails from a CSV file, but now we'll convert it to the modern conversational format for better fine-tuning with TRL's SFTTrainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c__CwiEmzhJ0"
      },
      "outputs": [],
      "source": [
        "def load_emails_from_csv(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load emails from a CSV file with semicolon delimiter.\"\"\"\n",
        "    df = pd.read_csv(file_path, sep=';')\n",
        "    logger.info(f\"Loaded {len(df)} emails from {file_path}\")\n",
        "    return df\n",
        "\n",
        "def prepare_training_data(emails_df: pd.DataFrame) -> List[Dict]:\n",
        "    \"\"\"Prepare training data in conversational format for SFTTrainer.\n",
        "\n",
        "    Creates direct style transfer pairs in conversational format:\n",
        "    - system: instruction on style transformation\n",
        "    - user: original AI-generated email\n",
        "    - assistant: styled version\n",
        "    \"\"\"\n",
        "    system_message = \"\"\"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
        "Your goal is to make the text feel more human-written with natural speech patterns.\"\"\"\n",
        "\n",
        "    training_samples = []\n",
        "\n",
        "    for _, row in emails_df.iterrows():\n",
        "        if pd.isna(row['body']) or pd.isna(row['body_ai']):\n",
        "            continue\n",
        "\n",
        "        # Create conversation in the format expected by TRL's SFTTrainer\n",
        "        sample = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": row['body_ai']},  # AI-generated email\n",
        "                {\"role\": \"assistant\", \"content\": row['body']}  # Custom style version\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        training_samples.append(sample)\n",
        "\n",
        "    logger.info(f\"Created {len(training_samples)} conversational training samples\")\n",
        "    return training_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDVh-N2kzhJ1",
        "outputId": "99f968d5-f7ce-4cdb-80f8-4a0ea0e6b170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample training conversation:\n",
            "system: Transform the given email into a custom-styled version that maintains the same content but uses a mo...\n",
            "user: Hi [Name],\\n\\nI'm [Your Name], founder of [Startup Name]. We're revolutionizing [industry] through [...\n",
            "assistant: Ahoy [Name],\\n\\nYer lookin' at [Your Name], fearsome captain of [Startup Name]. We be chartin' treac...\n",
            "Total training samples: 69\n"
          ]
        }
      ],
      "source": [
        "# Set the path to the CSV file\n",
        "EMAIL_CSV_PATH = '/content/humanize-LLM/data/manual_emails.csv'\n",
        "\n",
        "# Load and prepare the dataset\n",
        "emails_df = load_emails_from_csv(EMAIL_CSV_PATH)\n",
        "training_data = prepare_training_data(emails_df)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "dataset = Dataset.from_list(training_data)\n",
        "\n",
        "# Display a sample of the training data\n",
        "if len(dataset) > 0:\n",
        "    sample = dataset[0]\n",
        "    print(\"Sample training conversation:\")\n",
        "    for message in sample['messages']:\n",
        "        print(f\"{message['role']}: {message['content'][:100]}...\")\n",
        "    print(f\"Total training samples: {len(dataset)}\")\n",
        "else:\n",
        "    print(\"No training data found or prepared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7XNF6y0zhJ1"
      },
      "source": [
        "## Model Selection and QLoRA Configuration\n",
        "\n",
        "We'll use a smaller model suitable for Google Colab (Mistral-7B-Instruct-v0.2) with QLoRA for efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS = {\n",
        "    \"phi\": \"microsoft/phi-2\",  # 2.7B parameters\n",
        "    \"gemma\": \"google/gemma-2b\",  # 2B parameters\n",
        "    \"tiny_llama\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B parameters\n",
        "    \"mistral\": \"mistral/Mistral-7B\",  # 7B parameters\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def finetune_model(\n",
        "    dataset,\n",
        "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    merge_model=False,\n",
        "    push_to_hub=False,\n",
        "    hub_model_id=None,\n",
        "    hub_private=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tune a smaller language model using QLoRA for efficiency.\n",
        "    \"\"\"\n",
        "    # Define output directory\n",
        "    output_dir = f\"./results/{model_name.split('/')[-1]}-finetuned\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Load model and tokenizer with reduced precision for Colab\n",
        "    logger.info(f\"Loading base model: {model_name}\")\n",
        "    \n",
        "    # Configure quantization for memory efficiency\n",
        "    compute_dtype = torch.float16\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "    # Load model with quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    \n",
        "    # Configure LoRA for parameter-efficient tuning\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    )\n",
        "    \n",
        "    # Training arguments optimized for Colab - removed problematic parameter\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"adamw_torch\",\n",
        "        logging_steps=10,\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        push_to_hub=push_to_hub,\n",
        "        hub_model_id=hub_model_id\n",
        "    )\n",
        "    \n",
        "    # Initialize trainer\n",
        "    model.config.tokenizer = tokenizer\n",
        "    \n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        peft_config=peft_config,\n",
        "        max_seq_length=1024\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    logger.info(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    # Save the trained model\n",
        "    logger.info(f\"Saving model to {output_dir}\")\n",
        "    trainer.save_model(output_dir)\n",
        "    \n",
        "    # Optionally merge the model\n",
        "    if merge_model:\n",
        "        from peft import AutoPeftModelForCausalLM\n",
        "        logger.info(\"Merging LoRA adapters with base model...\")\n",
        "        merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "            output_dir, \n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        merged_model = merged_model.merge_and_unload()\n",
        "        \n",
        "        # Save the merged model\n",
        "        merged_output_dir = f\"{output_dir}-merged\"\n",
        "        os.makedirs(merged_output_dir, exist_ok=True)\n",
        "        merged_model.save_pretrained(merged_output_dir, safe_serialization=True)\n",
        "        tokenizer.save_pretrained(merged_output_dir)\n",
        "        logger.info(f\"Merged model saved to {merged_output_dir}\")\n",
        "    \n",
        "    # If hub_private is True but we can't use it directly in TrainingArguments,\n",
        "    # we can add a note about making the repo private manually\n",
        "    if hub_private and push_to_hub:\n",
        "        print(\"Note: To make your repository private, please visit the Hugging Face Hub website after uploading.\")\n",
        "        \n",
        "    return model, tokenizer, output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer, model_dir = finetune_model(\n",
        "    dataset=dataset,\n",
        "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    merge_model=False,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"username/tiny-llama-style-adapter\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLIfAqFtzhJ2"
      },
      "source": [
        "## Test and Evaluate the Fine-tuned Model\n",
        "\n",
        "Let's test our fine-tuned model with some example prompts and implement proper evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft9FV1oVsSrJ"
      },
      "outputs": [],
      "source": [
        "def generate_styled_text(prompt, model, tokenizer, max_new_tokens=200, temperature=0.7):\n",
        "    \"\"\"Generate text in the fine-tuned style across different model architectures\"\"\"\n",
        "    # Format the input properly for instruction models\n",
        "    formatted_prompt = f\"Transform the following text into a custom-styled version that feels more human-written:\\n\\n{prompt}\"\n",
        "    \n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate with specified parameters\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "    \n",
        "    # Decode the full output\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract only the response part (everything after the input prompt)\n",
        "    response = full_output[len(formatted_prompt):].strip()\n",
        "    \n",
        "    # Comprehensive cleanup for different model formats\n",
        "    \n",
        "    # Remove content within instruction tags (including the tags)\n",
        "    import re\n",
        "    response = re.sub(r'\\[INST\\].*?\\[/INST\\]', '', response, flags=re.DOTALL)\n",
        "    \n",
        "    # Remove any other model-specific formatting tags\n",
        "    cleanup_patterns = [\n",
        "        r'\\[INST\\]', r'\\[/INST\\]',           # Llama style\n",
        "        r'<s>', r'</s>',                      # Some models\n",
        "        r'<assistant>', r'</assistant>',      # Mistral style\n",
        "        r'<user>.*?</user>',                  # User prompts in some models\n",
        "        r'Assistant:', r'User:.*?\\n',         # Plain text format\n",
        "        r'Human:', r'AI:',                    # Alternative plain text format\n",
        "    ]\n",
        "    \n",
        "    for pattern in cleanup_patterns:\n",
        "        response = re.sub(pattern, '', response, flags=re.DOTALL)\n",
        "    \n",
        "    # Final cleanup of extra whitespace and newlines\n",
        "    response = re.sub(r'\\n{3,}', '\\n\\n', response)  # Replace excessive newlines\n",
        "    response = response.strip()\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQjAlOrGzhJ2",
        "outputId": "05b5b673-2abf-40c4-e543-15d6ee7f5bb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Prompt 1:\n",
            "Hello, I'm writing to inquire about your services. Could we schedule a call next week?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hello, I'm writing to inquire about your services. Could we schedule a call next week? [/INST] Ahoy! I be seekin' yer expertise! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Avast! I'll be needin' yer services! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] All hands ahoy! I'm in need of yer expertise! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Yo-ho-ho! I be seekin' yer services! When the tide's high enough for a parley?\\n\\nI'll bring the grog!,\\n[Your Name] [/INST] Avast! I'm look\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test Prompt 2:\n",
            "Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience.\n",
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience. [/INST] Avast HR!\\n\\nThis pirate submits his application for the software developer position! Me 5-year experience be worth more than a barrel of gold doubloons!\\n\\nYo-ho!,\\n[Your Name]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test Prompt 3:\n",
            "Team, please remember to submit your reports by Friday. The client is expecting our analysis.\n",
            "\n",
            "Custom-Style Response:\n",
            "[INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Team, please remember to submit your reports by Friday. The client is expecting our analysis. [/INST] All hands on deck! Reports due Friday or walk the plank! Client be waitin'! [/INST] Team,\\n\\nReports due Friday or walk the plank! Client be waitin'! No excuses!\\n\\nYo-ho-ho!,\\n[Your Name]\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_prompts = [\n",
        "    \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\",\n",
        "    \"Dear HR, I'm submitting my application for the software developer position. I have 5 years of experience.\",\n",
        "    \"Team, please remember to submit your reports by Friday. The client is expecting our analysis.\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nTest Prompt {i+1}:\\n{prompt}\")\n",
        "    styled_response = generate_styled_text(prompt, model, tokenizer)\n",
        "    print(f\"\\nCustom-Style Response:\\n{styled_response}\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOvS8DkwsSrJ"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "Let's evaluate our model on a subset of emails not used for training to assess its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F03I6HCJsSrJ",
        "outputId": "5cde9026-63b8-4bce-a7d3-67fefa191b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating model on 5 test samples...\n",
            "\n",
            "Original: Hi [Customers],\\n\\nWe're excited to launch [Feature]! Now you can...\\n\\nKey benefits:\\n- Benefit 1\\n- Benefit 2\\n\\nTry it now: [Link]...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Customers],\\n\\nWe're excited to launch [Feature]! Now you can...\\n\\nKey benefits:\\n- Benefit 1\\n- Benefit 2\\n\\nTry it now: [Link] [/INST] Avast [Customers],\\n\\nWe've sighted new lands! [Feature] be ready to plunder! Now ye can...\\n\\nBooty includes:\\n- Benefit 1\\n- Benefit 2\\n\\nSet sail now: [Link]\\n\\nYo-ho!,\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Customers],\\n\\nWe've sighted new lands! [Feature] be ready to plunder! Now ye can...\\n\\nBooty includes:\\n- Benefit 1\\n- Benefit 2\\n\\nSet sail now: [Link]\\n\\nSavvy?,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Team,\\n\\nQuick update:\\n✅ Completed: [Task A]\\n🛠 In Progress: [Task B]\\n⚠️ Blockers: [Issue C]\\n\\nLet's address in tomorrow's standup....\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Team,\\n\\nQuick update:\\n✅ Completed: [Task A]\\n🛠 In Progress: [Task B]\\n⚠️ Blockers: [Issue C]\\n\\nLet's address in tomorrow's standup. [/INST] All hands!\\n\\nRum Ration:\\n⚓ Completed: [Task A]\\n☠️ Battlin': [Task B]\\n🌊 Blockers: [Issue C]\\n\\nRum Line: Tomorrow's Yo-Ho-Ho! [/INST] Avast Team!\\n\\nRum Ration:\\n⚓ Completed: [Task A]\\n☠️ Battlin': [Task B]\\n🌊 Blockers: [Issue C]\\n\\nRum Line: Tomorrow's Yo-Ho-Ho!\\n\\nWalk the Plank!,\\n[Your Name]...\n",
            "\n",
            "Reference: All hands on deck!\\n\\n⚓ Completed: [Task A]\\n☠️ Battlin': [Task B]\\n🌊 Blockers: [Issue C]\\n\\nRum ration doubled for solutions!\\n\\nCap'n [Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Professor/Manager],\\n\\nI'm applying to [program/position] and wondered if you'd feel comfortable writing a recommendation letter highlighting my [specific skills]......\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Professor/Manager],\\n\\nI'm applying to [program/position] and wondered if you'd feel comfortable writing a recommendation letter highlighting my [specific skills]... [/INST] Avast [Professor/Manager],\\n\\nThis scalawag be applyin' for [program/position]! Could ye pen a few lines 'bout me [specific skills]? I'll name me ship after ye!\\n\\nYer humble deckhand,\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Professor/Manager],\\n\\nThis scalawag needs a letter o' marque for [program/position]! Could ye pen some kind words 'bout me [specific skills]? I'll name me firstborn after ye!\\n\\nYer humble deckhand,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Customer],\\n\\nHow's everything going with [Product]? Any challenges or questions?...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Customer],\\n\\nHow's everything going with [Product]? Any challenges or questions? [/INST] Avast [Customer],\\n\\nHow's [Product] treatin' ye? Any storms on the horizon? Need supplies?\\n\\nYer first mate's here to help!\\n\\n[Your Name]\"...\n",
            "\n",
            "Reference: Ahoy [Customer],\\n\\nHow's [Product] treatin' ye? Any storms on the horizon? Need supplies?\\n\\nYer quartermaster,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original: Hi [Client Name],\\n\\nHere's our weekly update:\\n1. Completed [Milestone A]\\n2. Currently working on [Task B]\\n3. Next steps: [Action Items]\\n\\nAny feedback?...\n",
            "\n",
            "Generated: [INST] Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\n",
            "\n",
            "Hi [Client Name],\\n\\nHere's our weekly update:\\n1. Completed [Milestone A]\\n2. Currently working on [Task B]\\n3. Next steps: [Action Items]\\n\\nAny feedback? [/INST] Ahoy [Client Name],\\n\\nShip's Log, Week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Blackbeard's ghost\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYo-ho!,\\n[Your Name] [/INST] Avast [Client Name],\\n\\nShip's Log, Week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Davy Jones' locker\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYo-ho-ho!,\\n[Your Name]...\n",
            "\n",
            "Reference: Avast [Client Name],\\n\\nShip's log, week [X]:\\n1. Plundered [Milestone A]\\n2. Battlin' [Task B] like Blackbeard's ghost\\n3. Next port: [Action Items]\\n\\nAny sea shanties to add?\\n\\nYer first mate,\\n[Your Name]...\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, tokenizer, test_samples=5):\n",
        "    \"\"\"Evaluate model performance on test samples with metrics\"\"\"\n",
        "    # Import necessary libraries for metrics\n",
        "    from rouge import Rouge\n",
        "    import numpy as np\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "    \n",
        "    # Initialize ROUGE\n",
        "    rouge = Rouge()\n",
        "    \n",
        "    # Use a subset of our dataset for testing\n",
        "    if len(dataset) <= test_samples:\n",
        "        test_indices = range(len(dataset))\n",
        "    else:\n",
        "        import random\n",
        "        test_indices = random.sample(range(len(dataset)), test_samples)\n",
        "\n",
        "    print(f\"\\nEvaluating model on {len(test_indices)} test samples...\")\n",
        "    \n",
        "    # Prepare to collect metrics\n",
        "    rouge_scores = []\n",
        "    bleu_scores = []\n",
        "\n",
        "    for idx in test_indices:\n",
        "        sample = dataset[idx]\n",
        "\n",
        "        # Extract original prompt and reference\n",
        "        original_text = sample['messages'][1]['content']  # user message\n",
        "        reference_text = sample['messages'][2]['content']  # assistant message\n",
        "\n",
        "        # Generate styled version using our improved function\n",
        "        generated_text = generate_styled_text(original_text, model, tokenizer)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        try:\n",
        "            # ROUGE scores\n",
        "            rouge_score = rouge.get_scores(generated_text, reference_text)[0]\n",
        "            rouge_scores.append({\n",
        "                'rouge-1': rouge_score['rouge-1']['f'],\n",
        "                'rouge-2': rouge_score['rouge-2']['f'],\n",
        "                'rouge-l': rouge_score['rouge-l']['f']\n",
        "            })\n",
        "            \n",
        "            # BLEU score (simple version)\n",
        "            bleu = sentence_bleu([reference_text.split()], generated_text.split(), weights=(0.25, 0.25, 0.25, 0.25))\n",
        "            bleu_scores.append(bleu)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not calculate metrics: {e}\")\n",
        "        \n",
        "        # Display results in a more readable format\n",
        "        print(f\"\\n{'='*20} Sample {idx+1} {'='*20}\")\n",
        "        print(f\"\\n📝 Original Text:\")\n",
        "        print(f\"{original_text}\")\n",
        "        \n",
        "        print(f\"\\n🤖 Generated Text:\")\n",
        "        print(f\"{generated_text}\")\n",
        "        \n",
        "        print(f\"\\n✓ Reference Text:\")\n",
        "        print(f\"{reference_text}\")\n",
        "        \n",
        "        # Print individual metrics for this sample\n",
        "        if rouge_scores:\n",
        "            print(f\"\\n📊 Metrics:\")\n",
        "            print(f\"ROUGE-1: {rouge_scores[-1]['rouge-1']:.4f}\")\n",
        "            print(f\"ROUGE-2: {rouge_scores[-1]['rouge-2']:.4f}\")\n",
        "            print(f\"ROUGE-L: {rouge_scores[-1]['rouge-l']:.4f}\")\n",
        "            print(f\"BLEU: {bleu_scores[-1]:.4f}\")\n",
        "        \n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "    \n",
        "    # Print average metrics\n",
        "    if rouge_scores:\n",
        "        avg_rouge1 = np.mean([score['rouge-1'] for score in rouge_scores])\n",
        "        avg_rouge2 = np.mean([score['rouge-2'] for score in rouge_scores])\n",
        "        avg_rougel = np.mean([score['rouge-l'] for score in rouge_scores])\n",
        "        avg_bleu = np.mean(bleu_scores)\n",
        "        \n",
        "        print(\"\\n📊 Average Metrics Across All Samples:\")\n",
        "        print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
        "        print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
        "        print(f\"ROUGE-L: {avg_rougel:.4f}\")\n",
        "        print(f\"BLEU: {avg_bleu:.4f}\")\n",
        "\n",
        "# Run evaluation (requires the improved generate_styled_text function from previous responses)\n",
        "# You may need to pip install rouge nltk first\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "except:\n",
        "    print(\"Note: Install required packages with: pip install rouge nltk\")\n",
        "\n",
        "evaluate_model(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deployment Options\n",
        "\n",
        "Now that we have a fine-tuned model, let's explore different options for using it in a production pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Push to Hugging Face Hub\n",
        "\n",
        "Pushing your model to Hugging Face Hub allows for easy sharing and access via their API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Hugging Face Hub (you'll need an account and API token)\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "login(token=userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def push_model_to_hub(model_path, repo_name, organization=None):\n",
        "    \"\"\"Push the fine-tuned model to Hugging Face Hub\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    \n",
        "    # Load the fine-tuned model\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    \n",
        "    # Optional: Merge weights for efficient inference\n",
        "    print(\"Merging adapter weights with base model...\")\n",
        "    merged_model = model.merge_and_unload()\n",
        "    \n",
        "    # Determine the full repo name\n",
        "    if organization:\n",
        "        full_repo_name = f\"{organization}/{repo_name}\"\n",
        "    else:\n",
        "        full_repo_name = repo_name\n",
        "        \n",
        "    print(f\"Pushing model to {full_repo_name}...\")\n",
        "    \n",
        "    # Push to hub\n",
        "    merged_model.push_to_hub(full_repo_name)\n",
        "    tokenizer.push_to_hub(full_repo_name)\n",
        "    \n",
        "    print(f\"Model successfully pushed to https://huggingface.co/{full_repo_name}\")\n",
        "    return full_repo_name\n",
        "\n",
        "# Uncomment to push your model\n",
        "repo_name = push_model_to_hub(\n",
        "    model_path=OUTPUT_DIR,\n",
        "    repo_name=\"custom-style-mistral-7b\",\n",
        "    organization=None  # Replace with your org name if applicable\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Model via Hugging Face API\n",
        "\n",
        "Once your model is on Hugging Face Hub, you can use it via their Inference API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def use_model_via_api(repo_id, prompt):\n",
        "    \"\"\"Use the model via Hugging Face Inference API\"\"\"\n",
        "    import requests\n",
        "    \n",
        "    # API endpoint\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{repo_id}\"\n",
        "    \n",
        "    # You need an API token with read access\n",
        "    headers = {\"Authorization\": \"Bearer YOUR_HF_TOKEN\"}  # Replace with your token\n",
        "    \n",
        "    # Prepare the payload - format as chat\n",
        "    system_message = \"Transform the given email into a custom-styled version that maintains the same content but uses a more personal, unique tone.\"\n",
        "    \n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        },\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 200,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Make the request\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example usage (uncomment to test)\n",
        "repo_id = \"Bormann/custom-style-mistral-7b\"  # Replace with your actual repo ID\n",
        "test_prompt = \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\"\n",
        "result = use_model_via_api(repo_id, test_prompt)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Local Inference Pipeline\n",
        "\n",
        "Here's how you can run inference locally with your fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_local_inference_pipeline(model_path):\n",
        "    \"\"\"Set up a pipeline for local inference\"\"\"\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import pipeline, AutoTokenizer\n",
        "    import torch\n",
        "    \n",
        "    print(\"Loading model for local inference...\")\n",
        "    \n",
        "    # Check if we're running on a GPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Load the model\n",
        "    # For a merged model:\n",
        "    if os.path.exists(os.path.join(model_path, \"pytorch_model.bin\")):\n",
        "        from transformers import AutoModelForCausalLM\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\n",
        "    # For a PEFT model:\n",
        "    else:\n",
        "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            device_map=device,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "        )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "def create_style_transfer_pipeline(model_path):\n",
        "    \"\"\"Create a simple pipeline for style transfer\"\"\"\n",
        "    model, tokenizer = setup_local_inference_pipeline(model_path)\n",
        "    \n",
        "    def style_transfer(text, max_length=200):\n",
        "        \"\"\"Transform text into the custom style\"\"\"\n",
        "        return generate_styled_text(text, model, tokenizer, max_new_tokens=max_length)\n",
        "    \n",
        "    return style_transfer\n",
        "\n",
        "# Example usage:\n",
        "# model_path = \"/path/to/your/model\"  # Use your actual model path\n",
        "# style_pipeline = create_style_transfer_pipeline(model_path)\n",
        "# \n",
        "# # Test the pipeline\n",
        "# original_text = \"Hello, I'm writing to inquire about your services. Could we schedule a call next week?\"\n",
        "# styled_text = style_pipeline(original_text)\n",
        "# print(f\"Original: {original_text}\")\n",
        "# print(f\"Styled: {styled_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration Into Your Pipeline\n",
        "\n",
        "Here are some tips for integrating your model into a production pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example_production_pipeline():\n",
        "    \"\"\"Example of how to integrate the style transfer model into a production pipeline\"\"\"\n",
        "    # Sample Python code for a basic pipeline - not meant to be run here\n",
        "    print(\"This is example code for a production pipeline:\")\n",
        "    \n",
        "    code_example = \"\"\"\n",
        "    import os\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "    from fastapi import FastAPI, Body\n",
        "    from pydantic import BaseModel\n",
        "\n",
        "    # Initialize FastAPI app\n",
        "    app = FastAPI()\n",
        "\n",
        "    # Initialize the model (run only once at startup)\n",
        "    MODEL_PATH = \"your-username/custom-style-model\"  # HF Hub path or local path\n",
        "    \n",
        "    # Choose loading method based on deployment option\n",
        "    if os.environ.get(\"USE_HF_API\") == \"True\":\n",
        "        # Option 1: Use Hugging Face Inference API\n",
        "        from huggingface_hub import InferenceClient\n",
        "        client = InferenceClient(token=os.environ.get(\"HF_TOKEN\"))\n",
        "        \n",
        "        def style_transfer(text):\n",
        "            system_message = \"Transform the given email into a custom-styled version.\"\n",
        "            messages = [{\"role\": \"system\", \"content\": system_message}, \n",
        "                      {\"role\": \"user\", \"content\": text}]\n",
        "            response = client.chat_completion(MODEL_PATH, messages)\n",
        "            return response.choices[0].message.content\n",
        "    else:\n",
        "        # Option 2: Run locally\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
        "        \n",
        "        def style_transfer(text):\n",
        "            system_message = \"Transform the given email into a custom-styled version.\"\n",
        "            messages = [{\"role\": \"system\", \"content\": system_message}, \n",
        "                      {\"role\": \"user\", \"content\": text}]\n",
        "            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            # Extract just the assistant's response\n",
        "            return response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    # Define request/response models\n",
        "    class StyleRequest(BaseModel):\n",
        "        text: str\n",
        "\n",
        "    class StyleResponse(BaseModel):\n",
        "        original: str\n",
        "        styled: str\n",
        "\n",
        "    # Define API endpoint\n",
        "    @app.post(\"/style-transfer/\", response_model=StyleResponse)\n",
        "    async def transform_style(request: StyleRequest):\n",
        "        styled_text = style_transfer(request.text)\n",
        "        return StyleResponse(original=request.text, styled=styled_text)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(code_example)\n",
        "\n",
        "# Show example pipeline code\n",
        "example_production_pipeline()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05d36e9a13b244469880979a29940def": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06c323a0068e47818e81390f8290eab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08fb02dfc3cb4926873600def54162dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a1eae16356c4b7198654b93d84a5bfc",
            "placeholder": "​",
            "style": "IPY_MODEL_c3c179a86176407eabf64648773512c4",
            "value": " 3/3 [01:10&lt;00:00, 23.08s/it]"
          }
        },
        "0ad07ffbab4e4829a3ff8013081f1d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d1b761e79848da86c8d712e224fd20",
            "placeholder": "​",
            "style": "IPY_MODEL_6806c0b84852473aa560798671c26b11",
            "value": " 69/69 [00:00&lt;00:00, 1040.53 examples/s]"
          }
        },
        "0f7120783c714a449593ec6f3daabdb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f992ef170aa4b4bb3f1c8b05db0d7ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11d152292b5d419db29b720af3ed17df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_475303ea81884c05a139800a90131ee8",
              "IPY_MODEL_a8b9cd1b1f814cba8904e80b53d174f6",
              "IPY_MODEL_db88865a74b14b2b90356136e20b005b"
            ],
            "layout": "IPY_MODEL_f2ff8fa153404f4e9d2462d90b4323ea"
          }
        },
        "16589004f6cf46399dea10351c03f75a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d59e6810b542f9a6993a3c851568b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_733056ec9ba44d8ea48c70e557261dcf",
            "placeholder": "​",
            "style": "IPY_MODEL_0f992ef170aa4b4bb3f1c8b05db0d7ed",
            "value": " 69/69 [00:00&lt;00:00, 882.94 examples/s]"
          }
        },
        "1891136a49214d3086e68468ce534eed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc5d158a5294292bac6e3b0dac9f1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a27fe7d4ba6e4343ba7a480a618e6a28",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b182bd03ca5c4173ab43bad62023d9a1",
            "value": 69
          }
        },
        "1d2643e1d4ec48bbb2a843eaa04aad55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_564c9338cc5a4b34a74d20037abd7157",
              "IPY_MODEL_d1badf74bf554c61bdf1540d66d49bb4",
              "IPY_MODEL_792456974b844899b8b838ee33c335bd"
            ],
            "layout": "IPY_MODEL_d71154e731084b7da8b0dda47561ff79"
          }
        },
        "2186b649bf2448faac5508f0f4c024bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22f5c46e907341dab0e6bda949b0ee13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c4859ee8e4a429ca489b3bf7144ec83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dcd656d76f249448f21c3423fce2b35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30021af32c1644a7a6b36ed7abb08dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db364d88095d4f4dab0754a548b01114",
              "IPY_MODEL_1cc5d158a5294292bac6e3b0dac9f1f4",
              "IPY_MODEL_16d59e6810b542f9a6993a3c851568b7"
            ],
            "layout": "IPY_MODEL_bbcb0db9b29f4813bd34fbb6346a1917"
          }
        },
        "41d23c8c4c014feda41ae704756b27fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46152fbfd14f42e8b979280920e2ca1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475303ea81884c05a139800a90131ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d36d4e59579046788804c2a5e4ca1cde",
            "placeholder": "​",
            "style": "IPY_MODEL_d34cd591241647d78d79ab6268102a0f",
            "value": "Truncating train dataset: 100%"
          }
        },
        "537906d718e2443aba58389f73ad47cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "560d6364ab9b4e03885ced53a65ca405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564c9338cc5a4b34a74d20037abd7157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560d6364ab9b4e03885ced53a65ca405",
            "placeholder": "​",
            "style": "IPY_MODEL_2c4859ee8e4a429ca489b3bf7144ec83",
            "value": "Applying chat template to train dataset: 100%"
          }
        },
        "5a1eae16356c4b7198654b93d84a5bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6806c0b84852473aa560798671c26b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "733056ec9ba44d8ea48c70e557261dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792456974b844899b8b838ee33c335bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46152fbfd14f42e8b979280920e2ca1b",
            "placeholder": "​",
            "style": "IPY_MODEL_996701bf1860407b99c9eb97891d121b",
            "value": " 69/69 [00:00&lt;00:00, 1581.44 examples/s]"
          }
        },
        "7c25715847ab45acab6ccb5de84ed7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3cd9397f1a419a8cea5d807e1273f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84530d834cf34728b943cbc1307ac3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d85ea79fd8b64749b87b3830532437e2",
              "IPY_MODEL_87e8fba41b88475a866234e92de246b9",
              "IPY_MODEL_08fb02dfc3cb4926873600def54162dd"
            ],
            "layout": "IPY_MODEL_bdbc774f19074ec48be5569382a70725"
          }
        },
        "85e7b03a3e164e44bf8183a5cbc3cd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c25715847ab45acab6ccb5de84ed7d0",
            "placeholder": "​",
            "style": "IPY_MODEL_935bd2febc66455ab1aeb6f41f0ddcde",
            "value": "Converting train dataset to ChatML: 100%"
          }
        },
        "87e8fba41b88475a866234e92de246b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5afcbfe728c4238999fa9d1b0e44cbe",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_537906d718e2443aba58389f73ad47cc",
            "value": 3
          }
        },
        "8ed49a1df23a462e8602c5904c56cbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c94effb3c5c74d6e89af78534c301b37",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05d36e9a13b244469880979a29940def",
            "value": 69
          }
        },
        "935bd2febc66455ab1aeb6f41f0ddcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "996701bf1860407b99c9eb97891d121b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0d1b761e79848da86c8d712e224fd20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a27fe7d4ba6e4343ba7a480a618e6a28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8b9cd1b1f814cba8904e80b53d174f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d564512ce9b64d2aa21b30feec02457c",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f7120783c714a449593ec6f3daabdb1",
            "value": 69
          }
        },
        "b182bd03ca5c4173ab43bad62023d9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5afcbfe728c4238999fa9d1b0e44cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbcb0db9b29f4813bd34fbb6346a1917": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdbc774f19074ec48be5569382a70725": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c179a86176407eabf64648773512c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4108b2bca78465a8f0ae9f1d74a5035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85e7b03a3e164e44bf8183a5cbc3cd25",
              "IPY_MODEL_8ed49a1df23a462e8602c5904c56cbc7",
              "IPY_MODEL_0ad07ffbab4e4829a3ff8013081f1d88"
            ],
            "layout": "IPY_MODEL_2dcd656d76f249448f21c3423fce2b35"
          }
        },
        "c94effb3c5c74d6e89af78534c301b37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbabf3a9cfbf4e888df1c8f11e003ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1badf74bf554c61bdf1540d66d49bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2186b649bf2448faac5508f0f4c024bd",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbabf3a9cfbf4e888df1c8f11e003ec4",
            "value": 69
          }
        },
        "d34cd591241647d78d79ab6268102a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d36d4e59579046788804c2a5e4ca1cde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d564512ce9b64d2aa21b30feec02457c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71154e731084b7da8b0dda47561ff79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d85ea79fd8b64749b87b3830532437e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16589004f6cf46399dea10351c03f75a",
            "placeholder": "​",
            "style": "IPY_MODEL_41d23c8c4c014feda41ae704756b27fe",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "db364d88095d4f4dab0754a548b01114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e3cd9397f1a419a8cea5d807e1273f5",
            "placeholder": "​",
            "style": "IPY_MODEL_22f5c46e907341dab0e6bda949b0ee13",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "db88865a74b14b2b90356136e20b005b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1891136a49214d3086e68468ce534eed",
            "placeholder": "​",
            "style": "IPY_MODEL_06c323a0068e47818e81390f8290eab5",
            "value": " 69/69 [00:00&lt;00:00, 2898.56 examples/s]"
          }
        },
        "f2ff8fa153404f4e9d2462d90b4323ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
